{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdce42d",
   "metadata": {},
   "source": [
    "# Projeto: Vanilla LLM — Zero-Shot Classification com Gutenberg\n",
    "\n",
    "**Objetivo:** implementar e comparar uma solução *zero-shot* usando modelos open-source e uma solução melhorada (RAG ou fine-tuning). Este notebook está pronto para ser executado no seu ambiente (Colab, local com GPU, etc.).\n",
    "\n",
    "**Estrutura:**\n",
    "1. Instalação de dependências\n",
    "2. Download e preparação dos textos do Project Gutenberg\n",
    "3. Criação do dataset (trechos + rótulos)\n",
    "4. Baseline Zero-Shot (transformers `zero-shot-classification` / NLI)\n",
    "5. Solução RAG (embeddings + FAISS + LLM para classificação usando contexto recuperado)\n",
    "6. (Opcional) Fine-tuning via LoRA (PEFT)\n",
    "7. Avaliação e comparação de métricas\n",
    "\n",
    "---\n",
    "\n",
    "**Observação importante:** este notebook contém código pronto para execução, porém neste ambiente o download direto da web pode estar bloqueado. Rode o notebook localmente ou no Google Colab para executar todos os passos. Se preferir, eu adapto este notebook para rodar em Colab (com configurações de GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71bb56d",
   "metadata": {},
   "source": [
    "## 1) Instalação de dependências\n",
    "\n",
    "Execute a célula abaixo para instalar bibliotecas necessárias. Use uma GPU (Colab com GPU ou máquina local) para etapas com modelos grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7ad40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0, sentence-transformers==2.1.0, sentence-transformers==2.2.0, sentence-transformers==2.2.1, sentence-transformers==2.2.2, sentence-transformers==2.3.0, sentence-transformers==2.3.1, sentence-transformers==2.4.0, sentence-transformers==2.5.0, sentence-transformers==2.5.1, sentence-transformers==2.6.0, sentence-transformers==2.6.1, sentence-transformers==2.7.0, sentence-transformers==3.0.0, sentence-transformers==3.0.1, sentence-transformers==3.1.0, sentence-transformers==3.1.1, sentence-transformers==3.2.0, sentence-transformers==3.2.1, sentence-transformers==3.3.0, sentence-transformers==3.3.1, sentence-transformers==3.4.0, sentence-transformers==3.4.1, sentence-transformers==4.0.0, sentence-transformers==4.0.1, sentence-transformers==4.0.2, sentence-transformers==4.1.0, sentence-transformers==5.0.0, sentence-transformers==5.1.0, sentence-transformers==5.1.1 and sentence-transformers==5.1.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Cannot install peft==0.0.1, peft==0.0.2, peft==0.1.0, peft==0.10.0, peft==0.11.0, peft==0.11.1, peft==0.12.0, peft==0.13.0, peft==0.13.1, peft==0.13.2, peft==0.14.0, peft==0.15.0, peft==0.15.1, peft==0.15.2, peft==0.16.0, peft==0.17.0, peft==0.17.1, peft==0.18.0, peft==0.2.0, peft==0.3.0, peft==0.4.0, peft==0.5.0, peft==0.6.0, peft==0.6.1, peft==0.6.2, peft==0.7.0, peft==0.7.1, peft==0.8.0, peft==0.8.1, peft==0.8.2 and peft==0.9.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.34.0\n",
    "!pip install -q sentence-transformers faiss-cpu datasets scikit-learn matplotlib pandas nbformat\n",
    "!pip install -q accelerate peft bitsandbytes safetensors\n",
    "!pip install -q gutenbergpy\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install tf-keras\n",
    "!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fbe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in /usr/local/lib/python3.11/site-packages (from tf-keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8487c089",
   "metadata": {},
   "source": [
    "## 2) Download e preparação dos textos do Project Gutenberg\n",
    "\n",
    "Neste exemplo usaremos trechos de alguns títulos (por exemplo: *Pride and Prejudice*, *Dracula*, *Treasure Island*, *The Time Machine*). O objetivo é extrair parágrafos/trechos de ~200–400 palavras, rotulá-los manualmente pelo gênero e gerar um CSV com colunas: `text`, `label`.\n",
    "\n",
    "O código abaixo tenta baixar usando `gutenbergpy`. Se preferir, você pode baixar os arquivos HTML/TXT manualmente e apontar para o diretório local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd58244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixado: gutenberg_texts/1342.txt\n",
      "Baixado: gutenberg_texts/345.txt\n",
      "Baixado: gutenberg_texts/120.txt\n",
      "Baixado: gutenberg_texts/35.txt\n",
      "1342.txt -> 780 parágrafos extraídos (entre 200-600 chars)\n",
      "345.txt -> 587 parágrafos extraídos (entre 200-600 chars)\n",
      "120.txt -> 583 parágrafos extraídos (entre 200-600 chars)\n",
      "35.txt -> 72 parágrafos extraídos (entre 200-600 chars)\n"
     ]
    }
   ],
   "source": [
    "# 2.a - Download e extração de trechos\n",
    "import os, re, random, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('gutenberg_texts')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# IDs do Gutenberg\n",
    "gutenberg_ids = {\n",
    "    'pride_and_prejudice': 1342,  # Pride and Prejudice\n",
    "    'dracula': 345,               # Dracula\n",
    "    'treasure_island': 120,       # Treasure Island\n",
    "    'the_time_machine': 35        # The Time Machine (exemplo)\n",
    "}\n",
    "\n",
    "def download_gutenberg(id, out_dir=DATA_DIR):\n",
    "    try:\n",
    "        import requests\n",
    "        url = f'https://www.gutenberg.org/files/{id}/{id}-0.txt'\n",
    "        r = requests.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            url = f'https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt'\n",
    "            r = requests.get(url, timeout=30)\n",
    "        text = r.text\n",
    "        path = out_dir / f'{id}.txt'\n",
    "        path.write_text(text, encoding='utf-8')\n",
    "        print('Baixado:', path)\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print('Falha ao baixar id', id, e)\n",
    "        return None\n",
    "\n",
    "# Baixar os textos\n",
    "for name, gid in gutenberg_ids.items():\n",
    "    download_gutenberg(gid)\n",
    "\n",
    "# Função de extração de parágrafos limpos\n",
    "def extract_paragraphs_from_file(path, min_len=200, max_len=600):\n",
    "    txt = path.read_text(encoding='utf-8', errors='ignore')\n",
    "    # remove cabeçalhos do Gutenberg (simples)\n",
    "    # busca início após '*** START' e fim antes de '*** END' se presente\n",
    "    start = txt.find('*** START')\n",
    "    if start != -1:\n",
    "        txt = txt[start:]\n",
    "    end = txt.find('*** END')\n",
    "    if end != -1:\n",
    "        txt = txt[:end]\n",
    "    # split by double newlines e limpar\n",
    "    paras = [p.strip().replace('\\n', ' ') for p in txt.split('\\n\\n') if p.strip()]\n",
    "    good = []\n",
    "    for p in paras:\n",
    "        if len(p) >= min_len and len(p) <= max_len:\n",
    "            good.append(' '.join(p.split()))\n",
    "    return good\n",
    "\n",
    "# Teste de extração\n",
    "for f in DATA_DIR.glob('*.txt'):\n",
    "    paras = extract_paragraphs_from_file(f)\n",
    "    print(f.name, '->', len(paras), 'parágrafos extraídos (entre 200-600 chars)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245514e8",
   "metadata": {},
   "source": [
    "### Observação\n",
    "Se o download automático falhar, baixe manualmente os textos do Project Gutenberg (format TXT) e coloque em `gutenberg_texts/`. Em seguida rode a célula acima para extrair parágrafos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a4907",
   "metadata": {},
   "source": [
    "## 3) Criar dataset rotulado\n",
    "\n",
    "A ideia aqui é criar um CSV com trechos e rótulos (genre). Você pode rotular manualmente os parágrafos ou usar regras heurísticas (p.ex. livro -> gênero conhecido). Exemplo: todos os parágrafos de 'dracula' => label='terror'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276a5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvo: gutenberg_dataset.csv linhas = 672\n"
     ]
    }
   ],
   "source": [
    "# 3.a - Montar CSV a partir dos arquivos baixados\n",
    "import csv, json\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path('gutenberg_texts')\n",
    "OUT_CSV = Path('gutenberg_dataset.csv')\n",
    "\n",
    "# mapa simples arquivo -> genero\n",
    "file_genre_map = {\n",
    "    '1342.txt': 'romance',   # pride and prejudice\n",
    "    '345.txt': 'terror',     # dracula\n",
    "    '120.txt': 'aventura',   # treasure island\n",
    "    '35.txt': 'scifi'        # the time machine\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for f in DATA_DIR.glob('*.txt'):\n",
    "    gid = f.name\n",
    "    genre = file_genre_map.get(gid, None)\n",
    "    if genre is None:\n",
    "        # nome alternativo chk by id prefix\n",
    "        for key, g in file_genre_map.items():\n",
    "            if key in gid:\n",
    "                genre = g\n",
    "                break\n",
    "    if genre is None:\n",
    "        continue\n",
    "    paras = extract_paragraphs_from_file(f)\n",
    "    # limite\n",
    "    paras = paras[:200]\n",
    "    for p in paras:\n",
    "        rows.append({'text': p, 'label': genre})\n",
    "\n",
    "# salvar CSV\n",
    "if rows:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    print('Dataset salvo:', OUT_CSV, 'linhas =', len(df))\n",
    "else:\n",
    "    print('Nenhum dado encontrado. Verifique se os textos foram baixados.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d497757",
   "metadata": {},
   "source": [
    "## 4) Classificação Zero-Shot Vanilla\n",
    "\n",
    "Usaremos a pipeline `zero-shot-classification` do HuggingFace que funciona via modelos NLI como `facebook/bart-large-mnli` (aberto). Este método é um baseline sólido para classificação sem treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf70741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado, linhas = 672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    aventura     0.2233    0.4694    0.3026        49\n",
      "     romance     0.5366    0.3284    0.4074        67\n",
      "       scifi     0.6250    0.1786    0.2778        28\n",
      "      terror     0.4167    0.3571    0.3846        56\n",
      "\n",
      "    accuracy                         0.3500       200\n",
      "   macro avg     0.4504    0.3334    0.3431       200\n",
      "weighted avg     0.4386    0.3500    0.3572       200\n",
      "\n",
      "Accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "# 4.a - Zero-shot baseline usando huggingface pipeline (NLI)\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# carregar dataset\n",
    "import os\n",
    "if not os.path.exists('gutenberg_dataset.csv'):\n",
    "    print('Arquivo gutenberg_dataset.csv não encontrado. Rode as células de download e criação do dataset.')\n",
    "else:\n",
    "    df = pd.read_csv('gutenberg_dataset.csv')\n",
    "    print('Dataset carregado, linhas =', len(df))\n",
    "\n",
    "# labels possíveis (defina conforme seus textos)\n",
    "candidate_labels = ['romance', 'terror', 'aventura', 'scifi']\n",
    "\n",
    "# criar pipeline\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli')\n",
    "\n",
    "# inferência (amostra reduzida para demo)\n",
    "Y_true = []\n",
    "Y_pred = []\n",
    "for i, row in df.sample(n=min(200, len(df)), random_state=42).iterrows():\n",
    "    text = row['text'][:1000]  # encurtar para demo\n",
    "    res = classifier(text, candidate_labels)\n",
    "    pred = res['labels'][0]\n",
    "    Y_true.append(row['label'])\n",
    "    Y_pred.append(pred)\n",
    "\n",
    "print(classification_report(Y_true, Y_pred, digits=4))\n",
    "print('Accuracy:', accuracy_score(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e986ff8",
   "metadata": {},
   "source": [
    "## 5) RAG — Retrieval-Augmented Classification\n",
    "\n",
    "Fluxo:\n",
    "1. Indexar trechos rotulados (Embedding + FAISS)\n",
    "2. Para cada texto a classificar, recuperar K trechos mais semelhantes\n",
    "3. Construir prompt que inclua os exemplos recuperados (texto + label)\n",
    "4. Enviar prompt para um LLM open-source e pedir a classificação\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66d74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (25.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (80.9.0)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel\n",
      "Successfully installed wheel-0.45.1\n",
      "Requirement already satisfied: llama-cpp-python in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.3.16)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from llama-cpp-python) (2.3.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aef5e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen2.5-1.5b-instruct-q4_k_m.gguf', <http.client.HTTPMessage at 0x1ff0b9150>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n",
    "urllib.request.urlretrieve(url, \"qwen2.5-1.5b-instruct-q4_k_m.gguf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08fe9014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/site-packages (25.1.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/site-packages (0.45.1)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.1.1\n",
      "    Uninstalling pip-25.1.1:\n",
      "      Successfully uninstalled pip-25.1.1\n",
      "Successfully installed pip-25.3\n",
      "Collecting cmake\n",
      "  Downloading cmake-4.2.0-py3-none-macosx_10_10_universal2.whl.metadata (6.5 kB)\n",
      "Downloading cmake-4.2.0-py3-none-macosx_10_10_universal2.whl (51.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: cmake\n",
      "Successfully installed cmake-4.2.0\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /Users/diogoluiscampanha/Library/Python/3.11/lib/python/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp311-cp311-macosx_13_0_x86_64.whl size=4425728 sha256=62fd96c167b32793716370cbc9c0d7a3ed8a32d792da31c4719ea54226e6f38f\n",
      "  Stored in directory: /private/var/folders/hj/fndxn9tj3hbc3kjsbb8v9wlh0000gn/T/pip-ephem-wheel-cache-5qaunhut/wheels/d8/5b/e5/a7d4b5765da347d314e8155197440c9995a962f8e4a5f52b23\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llama-cpp-python][llama-cpp-python]\n",
      "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install cmake\n",
    "!{sys.executable} -m pip install --no-cache-dir llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3417da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [00:52<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from llama_cpp import Llama\n",
    "\n",
    "##############################################\n",
    "# 1) Configurações básicas\n",
    "##############################################\n",
    "\n",
    "MODEL_PATH = \"qwen2.5-1.5b-instruct-q4_k_m.gguf\"\n",
    "LABELS = ['romance', 'terror', 'aventura', 'scifi']\n",
    "K = 3\n",
    "MAX_EXAMPLE_CHARS = 350\n",
    "\n",
    "##############################################\n",
    "# 2) Carregar dataset\n",
    "##############################################\n",
    "\n",
    "df = pd.read_csv(\"gutenberg_dataset.csv\")\n",
    "\n",
    "##############################################\n",
    "# 3) MODELO DE EMBEDDINGS + FAISS\n",
    "##############################################\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "index_df = df.sample(n=min(1000, len(df)), random_state=42).reset_index(drop=True)\n",
    "texts = index_df[\"text\"].tolist()\n",
    "embs = embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "d = embs.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4a02d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (AMD Radeon Pro 560) - 3184 MiB free\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from qwen2.5-1.5b-instruct-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2.5-1.5b-instruct\n",
      "llama_model_loader: - kv   3:                            general.version str              = v0.1\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-1.5b-instruct\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.8B\n",
      "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.04 GiB (5.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151643 ('<|endoftext|>')\n",
      "load:   - 151645 ('<|im_end|>')\n",
      "load:   - 151662 ('<|fim_pad|>')\n",
      "load:   - 151663 ('<|repo_name|>')\n",
      "load:   - 151664 ('<|file_sep|>')\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.78 B\n",
      "print_info: general.name     = qwen2.5-1.5b-instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 170 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_REPACK model buffer size =   596.53 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1059.89 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: AMD Radeon Pro 560\n",
      "ggml_metal_init: found device: Intel(R) HD Graphics 630\n",
      "ggml_metal_init: picking default device: AMD Radeon Pro 560\n",
      "ggml_metal_init: GPU name:   AMD Radeon Pro 560\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: simdgroup reduction   = false\n",
      "ggml_metal_init: simdgroup matrix mul. = false\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = false\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = false\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  4294.97 MB\n",
      "ggml_metal_init: loaded kernel_add                                 0x7f7b7d87c660 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                          0x7f7b9683ec10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                          0x7f7b983d5a80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                          0x7f7b8058beb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                          0x7f7b76dd3500 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                          0x7f7b96d9c810 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                          0x7f7b6f7cdd50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                          0x7f7b96d9c5a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4                          0x7f7b9ba0e7f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                   0x7f7b8020ea30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                   0x7f7b53e18250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                   0x7f7b76dd3270 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                   0x7f7b6f774030 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                   0x7f7b53e18430 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                   0x7f7bb50e5660 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                   0x7f7b80211990 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub                                 0x7f7b983d5c60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                          0x7f7b53e18610 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul                                 0x7f7b80204a20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                          0x7f7b76dd3940 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div                                 0x7f7b6f7e95f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_div_row_c4                          0x7f7b7053aaa0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_add_id                              0x7f7b53e187f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f32                          0x7f7b53e189d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_f16                          0x7f7b96d9cc10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i32                          0x7f7b9688ea50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_repeat_i16                          0x7f7b983d5e40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale                               0x7f7b96d994b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_scale_4                             0x7f7b9ba36970 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_clamp                               0x7f7b6f7a08d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_tanh                                0x7f7b7d8c84b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_relu                                0x7f7b6f767f60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sigmoid                             0x7f7b6ece2650 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu                                0x7f7b53e18d90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_4                              0x7f7b96d99690 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_erf                            0x7f7b9688ec30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                          0x7f7b983d6020 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick                          0x7f7b8020b920 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                        0x7f7b80215e80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu                                0x7f7b6f7830e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_silu_4                              0x7f7b8020b4f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_elu                                 0x7f7b8020dbb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_abs                                 0x7f7b6f787cf0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sgn                                 0x7f7b80211be0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_step                                0x7f7b9ba0eb90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_hardswish                           0x7f7b9688ee10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                         0x7f7b9ba25d00 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_exp                                 0x7f7b96d99870 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_soft_max_f16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f16_4                    (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_soft_max_f32_4                    (not supported)\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                       0x7f7b53e18bb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                     0x7f7b6f74b340 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                        0x7f7b983d6320 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                        0x7f7b96d9c9f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                       0x7f7b96d99b50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                       0x7f7b76dd3b20 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                       0x7f7b80212cc0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                       0x7f7b53e19070 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                       0x7f7b53e19250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                      0x7f7b9ba35950 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                       0x7f7b6f7abf40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                       0x7f7b9688eff0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                       0x7f7b9ba35b30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                       0x7f7b983d6500 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                       0x7f7b6f73c590 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                    0x7f7b7e818380 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                     0x7f7b9ba35d10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                    0x7f7b53e19430 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                      0x7f7b7e818560 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                      0x7f7b53e19610 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                      0x7f7b76dd3d00 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                      0x7f7b9688f1d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                     0x7f7b96d99d30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                     0x7f7b983d66e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                        0x7f7b9ba35ef0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                        0x7f7b9688f3b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                        0x7f7b8020c360 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                       0x7f7b6f7b00d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                       0x7f7b76dd3ee0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                       0x7f7b6f7d6500 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                       0x7f7b9ba360d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                       0x7f7b9688f590 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                     0x7f7b8020c540 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_rms_norm                          (not supported)\n",
      "ggml_metal_init: skipping kernel_rms_norm_mul                      (not supported)\n",
      "ggml_metal_init: skipping kernel_rms_norm_mul_add                  (not supported)\n",
      "ggml_metal_init: skipping kernel_l2_norm                           (not supported)\n",
      "ggml_metal_init: skipping kernel_group_norm                        (not supported)\n",
      "ggml_metal_init: loaded kernel_norm                                0x7f7b96d99f10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                        0x7f7b53e197f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                        0x7f7b6f7c8d50 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                  0x7f7b6f75e000 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                       0x7f7b76dd4340 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                       0x7f7b53e199d0 | th_max =  512 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_mul_mv_f32_f32                    (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                   0x7f7b983d68c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32                    (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                   0x7f7b6f7a9410 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_1row               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f32_l4                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_f16_f16                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_mxfp4_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_2           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_3           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_4           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_f16_f32_r1_5           (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_1_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q8_0_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_mxfp4_f32_r1_2         (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_mxfp4_f32_r1_3         (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_mxfp4_f32_r1_4         (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_mxfp4_f32_r1_5         (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q4_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q5_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_2          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_3          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_4          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_q6_K_f32_r1_5          (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_2        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_3        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_4        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_ext_iq4_nl_f32_r1_5        (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f32_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_f16_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q8_0_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_mxfp4_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q2_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q3_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q4_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q5_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_q6_K_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_xxs_f32             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq3_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq2_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_s_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq1_m_f32               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_nl_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_iq4_xs_f32              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f32_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_f16_f32                    (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_1_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q8_0_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_mxfp4_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_mxfp4_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q2_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q3_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q4_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q5_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_q6_K_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_xxs_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq3_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq2_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_s_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq1_m_f32                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_nl_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_iq4_xs_f32                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_map0_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_map1_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f32_f16                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_f16_f16                 (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_0_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_1_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_0_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_1_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q8_0_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_mxfp4_f16               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q2_K_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q3_K_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q4_K_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q5_K_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_q6_K_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xxs_f16             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_xs_f16              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_xxs_f16             (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq3_s_f16               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq2_s_f16               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_s_f16               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq1_m_f16               (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_nl_f16              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_iq4_xs_f16              (not supported)\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                       0x7f7b983d6aa0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                       0x7f7b9688f770 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                      0x7f7b983d6c80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                      0x7f7b76dd40c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                     0x7f7b76dd36e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                     0x7f7b983d6e60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                       0x7f7b6f7d8f70 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                       0x7f7b983d7040 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f16                          0x7f7b983d7220 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_f32                          0x7f7b9688f950 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                      0x7f7b76dd45f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                      0x7f7b6f733060 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32           0x7f7b6f731510 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32           0x7f7b76dd4ac0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_upscale_f32                         0x7f7b6f7436f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_f32                             0x7f7b8020d660 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                  0x7f7b53e19bb0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32              0x7f7b76dd4ca0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_arange_f32                          0x7f7b76dd4e80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                 0x7f7b76dd5060 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                0x7f7b8020d840 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                      0x7f7b53e19d90 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h64            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h80            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h96            (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h112           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h128           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h192           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk192_hv128    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_h256           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_f16_hk576_hv512    (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q4_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q5_1_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_q8_0_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h64        (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h96        (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h128       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h192       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_h256       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_f16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q4_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q5_1_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_q8_0_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_set_f32                             0x7f7b53e19f70 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_set_i32                             0x7f7b53e1a150 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                         0x7f7b9688fb30 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                         0x7f7b7f6716c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                         0x7f7b7f6718a0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                         0x7f7b76dd5250 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                        0x7f7b76dd48d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                        0x7f7b9ba362b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                        0x7f7b53e1a430 | th_max =  768 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                        0x7f7b6f7fe770 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                        0x7f7b6f7a82b0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                      0x7f7b96d9a0f0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                        0x7f7b53e1a800 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                        0x7f7b6f7572e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                        0x7f7b6f7a1210 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                        0x7f7b53e1a610 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                        0x7f7b76dd5550 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                        0x7f7b76dd5730 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                        0x7f7b76dd5910 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                        0x7f7b7f671a80 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                        0x7f7b6f73a6d0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                        0x7f7b76dd5af0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_concat                              0x7f7b76dd5e40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqr                                 0x7f7b7f671c60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sqrt                                0x7f7b53e1a9e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sin                                 0x7f7b7f671e40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_cos                                 0x7f7b7f672020 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_neg                                 0x7f7b6f7d0b00 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_reglu                               0x7f7b9688fd10 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_geglu                               0x7f7b6f7ffa40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_swiglu                              0x7f7b53e1ac60 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                          0x7f7b76dd6020 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_geglu_erf                           0x7f7b7f672200 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_geglu_quick                         0x7f7b53e1ae40 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_sum_rows                            0x7f7b6f761460 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_mean                                0x7f7b7f6723e0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_argmax                              0x7f7b76dd6200 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                     0x7f7b7f6725c0 | th_max = 1024 | th_width =   64\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                     0x7f7b7f6727a0 | th_max = 1024 | th_width =   64\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    56.00 MiB\n",
      "llama_kv_cache_unified: size =   56.00 MiB (  2048 cells,  28 layers,  1/1 seqs), K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2712\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   299.75 MiB\n",
      "llama_context: graph nodes  = 1070\n",
      "llama_context: graph splits = 114 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.head_count_kv': '2', 'qwen2.embedding_length': '1536', 'qwen2.context_length': '32768', 'general.type': 'model', 'qwen2.attention.head_count': '12', 'qwen2.feed_forward_length': '8960', 'general.architecture': 'qwen2', 'qwen2.block_count': '28', 'general.file_type': '15', 'general.size_label': '1.8B', 'tokenizer.ggml.add_bos_token': 'false', 'general.version': 'v0.1', 'general.finetune': 'qwen2.5-1.5b-instruct', 'general.name': 'qwen2.5-1.5b-instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3887.01 ms /   422 tokens (    9.21 ms per token,   108.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     244.06 ms /     4 runs   (   61.02 ms per token,    16.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    4136.34 ms /   426 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificação: [romance]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##############################################\n",
    "# 4) Carregar modelo LLaMA/Qwen em GGUF\n",
    "##############################################\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=0  # CPU only\n",
    ")\n",
    "\n",
    "##############################################\n",
    "# 5) Recuperação (RAG)\n",
    "##############################################\n",
    "\n",
    "def retrieve_examples(query, k=K):\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [\n",
    "        {\n",
    "            \"text\": index_df.loc[idx, \"text\"][:MAX_EXAMPLE_CHARS],\n",
    "            \"label\": index_df.loc[idx, \"label\"]\n",
    "        }\n",
    "        for idx in I[0]\n",
    "    ]\n",
    "\n",
    "##############################################\n",
    "# 6) Prompt curto e eficiente\n",
    "##############################################\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Você é um classificador de textos. \n",
    "Sua tarefa é identificar o gênero literário do texto abaixo.\n",
    "\n",
    "INSTRUÇÕES IMPORTANTES:\n",
    "- Responda SOMENTE com uma das categorias: {labels}.\n",
    "- Não repita os exemplos.\n",
    "- Não escreva explicações.\n",
    "- Não escreva nada além do gênero final.\n",
    "\n",
    "EXEMPLOS DE REFERÊNCIA:\n",
    "{examples}\n",
    "\n",
    "TEXTO:\n",
    "{query}\n",
    "\n",
    "RESPOSTA APENAS COM O GÊNERO:\n",
    "\"\"\"\n",
    "\n",
    "##############################################\n",
    "# 7) Classificação com modelo GGUF\n",
    "##############################################\n",
    "\n",
    "def classify(query):\n",
    "    examples = retrieve_examples(query)\n",
    "    examples_str = \"\\n\\n\".join([\n",
    "        f\"[TEXTO]: {e['text']}\\n[LABEL]: {e['label']}\"\n",
    "        for e in examples\n",
    "    ])\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        labels=\", \".join(LABELS),\n",
    "        examples=examples_str,\n",
    "        query=query[:1200]\n",
    "    )\n",
    "\n",
    "    out = llm(\n",
    "        prompt,\n",
    "        max_tokens=10,\n",
    "        stop=[\"\\n\"]\n",
    "    )\n",
    "\n",
    "    resp = out[\"choices\"][0][\"text\"].strip()\n",
    "    return resp\n",
    "\n",
    "##############################################\n",
    "# 8) Teste\n",
    "##############################################\n",
    "\n",
    "sample = df.sample(1, random_state=1).iloc[0][\"text\"]\n",
    "print(\"Classificação:\", classify(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56608031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 99 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3291.76 ms /   254 tokens (   12.96 ms per token,    77.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     545.15 ms /     4 runs   (  136.29 ms per token,     7.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3843.50 ms /   258 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3255.22 ms /   301 tokens (   10.81 ms per token,    92.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     435.11 ms /     6 runs   (   72.52 ms per token,    13.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    3696.83 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3627.39 ms /   323 tokens (   11.23 ms per token,    89.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.88 ms /     6 runs   (   52.65 ms per token,    18.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.22 ms /   329 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2469.00 ms /   265 tokens (    9.32 ms per token,   107.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     214.97 ms /     4 runs   (   53.74 ms per token,    18.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    2688.18 ms /   269 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2456.56 ms /   239 tokens (   10.28 ms per token,    97.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     512.80 ms /     9 runs   (   56.98 ms per token,    17.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.46 ms /   248 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 358 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3544.95 ms /   358 tokens (    9.90 ms per token,   100.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     223.31 ms /     4 runs   (   55.83 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    3772.83 ms /   362 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2657.40 ms /   304 tokens (    8.74 ms per token,   114.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     427.34 ms /     4 runs   (  106.83 ms per token,     9.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.72 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2807.50 ms /   304 tokens (    9.24 ms per token,   108.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     335.70 ms /     6 runs   (   55.95 ms per token,    17.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.96 ms /   310 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4270.33 ms /   357 tokens (   11.96 ms per token,    83.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     294.34 ms /     4 runs   (   73.58 ms per token,    13.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    4569.37 ms /   361 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 104 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3203.84 ms /   353 tokens (    9.08 ms per token,   110.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     114.90 ms /     2 runs   (   57.45 ms per token,    17.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    3322.15 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 99 prefix-match hit, remaining 371 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4320.29 ms /   371 tokens (   11.64 ms per token,    85.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     514.04 ms /     5 runs   (  102.81 ms per token,     9.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4839.67 ms /   376 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 324 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2833.30 ms /   324 tokens (    8.74 ms per token,   114.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     538.34 ms /     9 runs   (   59.82 ms per token,    16.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3379.65 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3485.64 ms /   269 tokens (   12.96 ms per token,    77.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     486.71 ms /     8 runs   (   60.84 ms per token,    16.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    3979.52 ms /   277 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2603.69 ms /   288 tokens (    9.04 ms per token,   110.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     237.80 ms /     4 runs   (   59.45 ms per token,    16.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    2846.07 ms /   292 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3004.07 ms /   338 tokens (    8.89 ms per token,   112.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     522.79 ms /     9 runs   (   58.09 ms per token,    17.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    3534.83 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 394 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3643.53 ms /   394 tokens (    9.25 ms per token,   108.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     300.33 ms /     5 runs   (   60.07 ms per token,    16.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.44 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3533.85 ms /   390 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     232.64 ms /     4 runs   (   58.16 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    3770.61 ms /   394 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3373.55 ms /   372 tokens (    9.07 ms per token,   110.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     418.41 ms /     5 runs   (   83.68 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3797.38 ms /   377 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2712.48 ms /   304 tokens (    8.92 ms per token,   112.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     213.32 ms /     4 runs   (   53.33 ms per token,    18.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2929.72 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 303 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2787.68 ms /   303 tokens (    9.20 ms per token,   108.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     374.64 ms /     6 runs   (   62.44 ms per token,    16.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    3168.17 ms /   309 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 101 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2538.04 ms /   268 tokens (    9.47 ms per token,   105.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     497.22 ms /     8 runs   (   62.15 ms per token,    16.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3042.57 ms /   276 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2204.24 ms /   240 tokens (    9.18 ms per token,   108.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     560.64 ms /     9 runs   (   62.29 ms per token,    16.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    2773.08 ms /   249 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3123.12 ms /   357 tokens (    8.75 ms per token,   114.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     306.54 ms /     5 runs   (   61.31 ms per token,    16.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3434.89 ms /   362 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2551.72 ms /   275 tokens (    9.28 ms per token,   107.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     249.56 ms /     4 runs   (   62.39 ms per token,    16.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    2806.00 ms /   279 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2380.76 ms /   254 tokens (    9.37 ms per token,   106.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     235.68 ms /     4 runs   (   58.92 ms per token,    16.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    2620.52 ms /   258 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 329 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3029.40 ms /   329 tokens (    9.21 ms per token,   108.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.29 ms /     4 runs   (   58.32 ms per token,    17.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    3267.61 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2703.85 ms /   304 tokens (    8.89 ms per token,   112.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.72 ms /     4 runs   (   58.43 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    2941.64 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 360 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3353.30 ms /   360 tokens (    9.31 ms per token,   107.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     357.04 ms /     6 runs   (   59.51 ms per token,    16.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3716.34 ms /   366 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 403 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3595.97 ms /   403 tokens (    8.92 ms per token,   112.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     231.17 ms /     4 runs   (   57.79 ms per token,    17.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3832.36 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3006.94 ms /   319 tokens (    9.43 ms per token,   106.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     295.62 ms /     5 runs   (   59.12 ms per token,    16.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    3308.16 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3311.62 ms /   361 tokens (    9.17 ms per token,   109.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.00 ms /     6 runs   (   62.00 ms per token,    16.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3689.52 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2571.94 ms /   288 tokens (    8.93 ms per token,   111.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     526.95 ms /     9 runs   (   58.55 ms per token,    17.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3106.79 ms /   297 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3475.23 ms /   387 tokens (    8.98 ms per token,   111.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     226.09 ms /     4 runs   (   56.52 ms per token,    17.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    3705.78 ms /   391 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2885.80 ms /   317 tokens (    9.10 ms per token,   109.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     405.47 ms /     6 runs   (   67.58 ms per token,    14.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3297.48 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2616.18 ms /   288 tokens (    9.08 ms per token,   110.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     241.52 ms /     4 runs   (   60.38 ms per token,    16.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    2862.35 ms /   292 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 296 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2690.84 ms /   296 tokens (    9.09 ms per token,   110.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     560.79 ms /     9 runs   (   62.31 ms per token,    16.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3259.00 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3142.88 ms /   354 tokens (    8.88 ms per token,   112.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     287.42 ms /     5 runs   (   57.48 ms per token,    17.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3435.44 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2838.45 ms /   317 tokens (    8.95 ms per token,   111.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     343.39 ms /     6 runs   (   57.23 ms per token,    17.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    3187.64 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 101 prefix-match hit, remaining 350 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3139.73 ms /   350 tokens (    8.97 ms per token,   111.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     266.86 ms /     4 runs   (   66.71 ms per token,    14.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3411.56 ms /   354 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3220.70 ms /   338 tokens (    9.53 ms per token,   104.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     296.95 ms /     5 runs   (   59.39 ms per token,    16.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3523.56 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3141.95 ms /   344 tokens (    9.13 ms per token,   109.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.43 ms /     4 runs   (   58.36 ms per token,    17.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3380.46 ms /   348 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2840.74 ms /   298 tokens (    9.53 ms per token,   104.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     392.16 ms /     6 runs   (   65.36 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3239.26 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2212.84 ms /   250 tokens (    8.85 ms per token,   112.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     276.38 ms /     4 runs   (   69.09 ms per token,    14.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2493.23 ms /   254 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 264 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2399.80 ms /   264 tokens (    9.09 ms per token,   110.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     237.70 ms /     4 runs   (   59.43 ms per token,    16.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    2642.32 ms /   268 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2812.44 ms /   301 tokens (    9.34 ms per token,   107.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     525.04 ms /     9 runs   (   58.34 ms per token,    17.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3345.83 ms /   310 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2132.78 ms /   234 tokens (    9.11 ms per token,   109.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.60 ms /     4 runs   (   55.65 ms per token,    17.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    2359.85 ms /   238 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2931.56 ms /   326 tokens (    8.99 ms per token,   111.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     456.18 ms /     8 runs   (   57.02 ms per token,    17.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3394.89 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 382 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3388.29 ms /   382 tokens (    8.87 ms per token,   112.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.84 ms /     7 runs   (   53.26 ms per token,    18.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    3767.28 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2586.89 ms /   290 tokens (    8.92 ms per token,   112.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.29 ms /     4 runs   (   55.57 ms per token,    17.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2813.84 ms /   294 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2308.26 ms /   258 tokens (    8.95 ms per token,   111.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     262.59 ms /     4 runs   (   65.65 ms per token,    15.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    2575.49 ms /   262 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2422.13 ms /   268 tokens (    9.04 ms per token,   110.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     516.06 ms /     9 runs   (   57.34 ms per token,    17.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    2946.12 ms /   277 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3090.33 ms /   352 tokens (    8.78 ms per token,   113.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     281.74 ms /     5 runs   (   56.35 ms per token,    17.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3377.44 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2639.20 ms /   286 tokens (    9.23 ms per token,   108.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     283.58 ms /     5 runs   (   56.72 ms per token,    17.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    2928.07 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 101 prefix-match hit, remaining 327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2922.30 ms /   327 tokens (    8.94 ms per token,   111.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.74 ms /     6 runs   (   55.62 ms per token,    17.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    3262.56 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2329.65 ms /   254 tokens (    9.17 ms per token,   109.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     464.91 ms /     9 runs   (   51.66 ms per token,    19.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    2801.08 ms /   263 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2113.89 ms /   234 tokens (    9.03 ms per token,   110.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     193.37 ms /     3 runs   (   64.45 ms per token,    15.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    2310.74 ms /   237 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3175.56 ms /   352 tokens (    9.02 ms per token,   110.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     293.48 ms /     5 runs   (   58.70 ms per token,    17.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    3474.93 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 359 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3232.89 ms /   359 tokens (    9.01 ms per token,   111.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.09 ms /     4 runs   (   55.52 ms per token,    18.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3459.78 ms /   363 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2726.66 ms /   295 tokens (    9.24 ms per token,   108.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     394.36 ms /     7 runs   (   56.34 ms per token,    17.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3127.65 ms /   302 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 263 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2391.94 ms /   263 tokens (    9.09 ms per token,   109.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     291.85 ms /     4 runs   (   72.96 ms per token,    13.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    2688.61 ms /   267 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2303.99 ms /   238 tokens (    9.68 ms per token,   103.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     513.46 ms /     9 runs   (   57.05 ms per token,    17.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    2825.33 ms /   247 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3993.06 ms /   356 tokens (   11.22 ms per token,    89.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     287.46 ms /     5 runs   (   57.49 ms per token,    17.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    4286.19 ms /   361 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 278 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2688.33 ms /   278 tokens (    9.67 ms per token,   103.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.38 ms /     5 runs   (   69.88 ms per token,    14.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3043.46 ms /   283 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3313.22 ms /   316 tokens (   10.48 ms per token,    95.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     228.34 ms /     4 runs   (   57.09 ms per token,    17.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    3546.91 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3031.23 ms /   348 tokens (    8.71 ms per token,   114.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     554.57 ms /     9 runs   (   61.62 ms per token,    16.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3593.88 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3069.91 ms /   352 tokens (    8.72 ms per token,   114.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.38 ms /     6 runs   (   55.56 ms per token,    18.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3409.28 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2957.71 ms /   327 tokens (    9.04 ms per token,   110.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     250.98 ms /     4 runs   (   62.75 ms per token,    15.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    3213.75 ms /   331 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3175.63 ms /   354 tokens (    8.97 ms per token,   111.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.02 ms /     5 runs   (   57.60 ms per token,    17.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    3469.13 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2797.03 ms /   312 tokens (    8.96 ms per token,   111.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     230.80 ms /     4 runs   (   57.70 ms per token,    17.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3032.76 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2836.17 ms /   313 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     223.25 ms /     4 runs   (   55.81 ms per token,    17.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    3064.45 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3155.60 ms /   325 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     553.63 ms /     9 runs   (   61.51 ms per token,    16.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3716.23 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 382 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3647.16 ms /   382 tokens (    9.55 ms per token,   104.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     285.99 ms /     5 runs   (   57.20 ms per token,    17.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    3938.56 ms /   387 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 386 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3395.06 ms /   386 tokens (    8.80 ms per token,   113.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     286.04 ms /     5 runs   (   57.21 ms per token,    17.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    3687.02 ms /   391 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 337 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3114.53 ms /   337 tokens (    9.24 ms per token,   108.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     249.50 ms /     4 runs   (   62.38 ms per token,    16.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3369.07 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2911.59 ms /   315 tokens (    9.24 ms per token,   108.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     327.92 ms /     6 runs   (   54.65 ms per token,    18.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3245.76 ms /   321 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2710.73 ms /   298 tokens (    9.10 ms per token,   109.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     251.67 ms /     4 runs   (   62.92 ms per token,    15.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    2967.54 ms /   302 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2854.04 ms /   313 tokens (    9.12 ms per token,   109.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     589.82 ms /     9 runs   (   65.54 ms per token,    15.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3451.11 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 296 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2729.61 ms /   296 tokens (    9.22 ms per token,   108.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     505.02 ms /     9 runs   (   56.11 ms per token,    17.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    3242.57 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2666.01 ms /   294 tokens (    9.07 ms per token,   110.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     362.06 ms /     6 runs   (   60.34 ms per token,    16.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.96 ms /   300 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2249.92 ms /   237 tokens (    9.49 ms per token,   105.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     250.22 ms /     4 runs   (   62.55 ms per token,    15.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    2504.29 ms /   241 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2706.94 ms /   301 tokens (    8.99 ms per token,   111.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     403.32 ms /     7 runs   (   57.62 ms per token,    17.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.96 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2775.60 ms /   308 tokens (    9.01 ms per token,   110.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     367.60 ms /     6 runs   (   61.27 ms per token,    16.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3149.49 ms /   314 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2902.48 ms /   311 tokens (    9.33 ms per token,   107.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     548.89 ms /     9 runs   (   60.99 ms per token,    16.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3459.94 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3132.19 ms /   344 tokens (    9.11 ms per token,   109.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     225.61 ms /     4 runs   (   56.40 ms per token,    17.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3362.91 ms /   348 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2724.22 ms /   297 tokens (    9.17 ms per token,   109.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     527.66 ms /     9 runs   (   58.63 ms per token,    17.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3259.87 ms /   306 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2664.33 ms /   299 tokens (    8.91 ms per token,   112.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     507.26 ms /     9 runs   (   56.36 ms per token,    17.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3180.03 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2517.71 ms /   270 tokens (    9.32 ms per token,   107.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.18 ms /     6 runs   (   54.36 ms per token,    18.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    2849.58 ms /   276 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2680.31 ms /   295 tokens (    9.09 ms per token,   110.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     483.06 ms /     9 runs   (   53.67 ms per token,    18.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    3170.86 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2362.35 ms /   235 tokens (   10.05 ms per token,    99.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =     523.70 ms /     9 runs   (   58.19 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.06 ms /   244 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2778.58 ms /   311 tokens (    8.93 ms per token,   111.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     208.84 ms /     4 runs   (   52.21 ms per token,    19.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    2991.90 ms /   315 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2920.65 ms /   316 tokens (    9.24 ms per token,   108.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.48 ms /     6 runs   (   63.25 ms per token,    15.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3306.88 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3176.72 ms /   342 tokens (    9.29 ms per token,   107.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     279.23 ms /     5 runs   (   55.85 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    3462.01 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3673.22 ms /   412 tokens (    8.92 ms per token,   112.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     444.90 ms /     8 runs   (   55.61 ms per token,    17.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    4125.57 ms /   420 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2798.32 ms /   289 tokens (    9.68 ms per token,   103.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     553.85 ms /     9 runs   (   61.54 ms per token,    16.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3360.08 ms /   298 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2297.04 ms /   253 tokens (    9.08 ms per token,   110.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     518.43 ms /     9 runs   (   57.60 ms per token,    17.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    2822.65 ms /   262 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2673.00 ms /   295 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     223.38 ms /     4 runs   (   55.84 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    2901.17 ms /   299 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3081.24 ms /   326 tokens (    9.45 ms per token,   105.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     556.81 ms /     9 runs   (   61.87 ms per token,    16.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    3646.06 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 400 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3750.11 ms /   400 tokens (    9.38 ms per token,   106.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     214.67 ms /     4 runs   (   53.67 ms per token,    18.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    3969.42 ms /   404 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 331 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3086.51 ms /   331 tokens (    9.32 ms per token,   107.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     239.24 ms /     4 runs   (   59.81 ms per token,    16.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3330.53 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2464.01 ms /   241 tokens (   10.22 ms per token,    97.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     487.04 ms /     9 runs   (   54.12 ms per token,    18.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    2958.78 ms /   250 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4002.92 ms /   353 tokens (   11.34 ms per token,    88.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     435.00 ms /     4 runs   (  108.75 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4444.15 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 382 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3743.52 ms /   382 tokens (    9.80 ms per token,   102.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     224.87 ms /     4 runs   (   56.22 ms per token,    17.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    3973.70 ms /   386 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3268.05 ms /   347 tokens (    9.42 ms per token,   106.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     287.50 ms /     5 runs   (   57.50 ms per token,    17.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3561.33 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3102.78 ms /   321 tokens (    9.67 ms per token,   103.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     279.25 ms /     5 runs   (   55.85 ms per token,    17.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    3387.81 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2576.59 ms /   274 tokens (    9.40 ms per token,   106.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     789.82 ms /     4 runs   (  197.45 ms per token,     5.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3372.73 ms /   278 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6820.69 ms /   392 tokens (   17.40 ms per token,    57.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     485.15 ms /     4 runs   (  121.29 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    7311.62 ms /   396 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4826.08 ms /   269 tokens (   17.94 ms per token,    55.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     209.07 ms /     4 runs   (   52.27 ms per token,    19.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    5041.23 ms /   273 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4802.70 ms /   323 tokens (   14.87 ms per token,    67.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     217.81 ms /     4 runs   (   54.45 ms per token,    18.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    5025.33 ms /   327 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 365 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3409.36 ms /   365 tokens (    9.34 ms per token,   107.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.68 ms /     5 runs   (   60.94 ms per token,    16.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    3719.41 ms /   370 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4844.62 ms /   314 tokens (   15.43 ms per token,    64.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     568.95 ms /     5 runs   (  113.79 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    5420.54 ms /   319 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5761.82 ms /   377 tokens (   15.28 ms per token,    65.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.76 ms /     5 runs   (   61.55 ms per token,    16.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    6075.04 ms /   382 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3900.60 ms /   308 tokens (   12.66 ms per token,    78.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     549.88 ms /     9 runs   (   61.10 ms per token,    16.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    4458.99 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2595.86 ms /   270 tokens (    9.61 ms per token,   104.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     223.73 ms /     4 runs   (   55.93 ms per token,    17.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    2824.34 ms /   274 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2996.20 ms /   338 tokens (    8.86 ms per token,   112.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     564.32 ms /     9 runs   (   62.70 ms per token,    15.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3568.66 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2426.46 ms /   261 tokens (    9.30 ms per token,   107.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     226.82 ms /     4 runs   (   56.70 ms per token,    17.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    2658.05 ms /   265 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2573.55 ms /   277 tokens (    9.29 ms per token,   107.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     521.28 ms /     9 runs   (   57.92 ms per token,    17.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3102.64 ms /   286 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2963.57 ms /   323 tokens (    9.18 ms per token,   108.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     320.87 ms /     5 runs   (   64.17 ms per token,    15.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    3290.31 ms /   328 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2581.02 ms /   269 tokens (    9.59 ms per token,   104.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     235.56 ms /     4 runs   (   58.89 ms per token,    16.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    2821.57 ms /   273 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2699.94 ms /   289 tokens (    9.34 ms per token,   107.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.14 ms /     5 runs   (   65.83 ms per token,    15.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    3034.34 ms /   294 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2860.84 ms /   316 tokens (    9.05 ms per token,   110.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     531.55 ms /     9 runs   (   59.06 ms per token,    16.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    3400.39 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 334 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3100.78 ms /   334 tokens (    9.28 ms per token,   107.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     338.87 ms /     6 runs   (   56.48 ms per token,    17.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    3445.96 ms /   340 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2890.60 ms /   309 tokens (    9.35 ms per token,   106.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.83 ms /     4 runs   (   59.21 ms per token,    16.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    3132.80 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2915.32 ms /   300 tokens (    9.72 ms per token,   102.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     226.75 ms /     4 runs   (   56.69 ms per token,    17.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    3147.07 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2919.37 ms /   327 tokens (    8.93 ms per token,   112.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     519.53 ms /     9 runs   (   57.73 ms per token,    17.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3447.10 ms /   336 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 402 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3552.54 ms /   402 tokens (    8.84 ms per token,   113.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     217.16 ms /     4 runs   (   54.29 ms per token,    18.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3774.28 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 283 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2709.23 ms /   283 tokens (    9.57 ms per token,   104.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     229.45 ms /     4 runs   (   57.36 ms per token,    17.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    2943.25 ms /   287 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2989.96 ms /   326 tokens (    9.17 ms per token,   109.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     534.89 ms /     9 runs   (   59.43 ms per token,    16.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3532.68 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 291 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2817.16 ms /   291 tokens (    9.68 ms per token,   103.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     227.96 ms /     4 runs   (   56.99 ms per token,    17.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3050.08 ms /   295 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3544.91 ms /   377 tokens (    9.40 ms per token,   106.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     230.92 ms /     4 runs   (   57.73 ms per token,    17.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3780.54 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3337.04 ms /   346 tokens (    9.64 ms per token,   103.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     298.05 ms /     5 runs   (   59.61 ms per token,    16.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    3640.66 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 368 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3361.21 ms /   368 tokens (    9.13 ms per token,   109.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =     292.17 ms /     5 runs   (   58.43 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3658.91 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2651.16 ms /   286 tokens (    9.27 ms per token,   107.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     211.93 ms /     4 runs   (   52.98 ms per token,    18.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    2867.83 ms /   290 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 291 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2874.62 ms /   291 tokens (    9.88 ms per token,   101.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.30 ms /     6 runs   (   58.55 ms per token,    17.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3231.67 ms /   297 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 375 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3464.00 ms /   375 tokens (    9.24 ms per token,   108.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     700.18 ms /     4 runs   (  175.04 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    4169.64 ms /   379 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3298.35 ms /   339 tokens (    9.73 ms per token,   102.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     211.97 ms /     4 runs   (   52.99 ms per token,    18.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    3514.96 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3932.65 ms /   344 tokens (   11.43 ms per token,    87.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     220.62 ms /     4 runs   (   55.15 ms per token,    18.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    4158.38 ms /   348 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2898.70 ms /   307 tokens (    9.44 ms per token,   105.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     229.64 ms /     4 runs   (   57.41 ms per token,    17.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3133.25 ms /   311 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 296 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2888.15 ms /   296 tokens (    9.76 ms per token,   102.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.19 ms /     4 runs   (   55.55 ms per token,    18.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3115.62 ms /   300 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2516.34 ms /   265 tokens (    9.50 ms per token,   105.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.54 ms /     4 runs   (   59.13 ms per token,    16.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    2757.73 ms /   269 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4508.79 ms /   330 tokens (   13.66 ms per token,    73.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     665.57 ms /     4 runs   (  166.39 ms per token,     6.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    5180.26 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2699.52 ms /   309 tokens (    8.74 ms per token,   114.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     504.62 ms /     9 runs   (   56.07 ms per token,    17.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3211.96 ms /   318 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5085.54 ms /   347 tokens (   14.66 ms per token,    68.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     227.44 ms /     4 runs   (   56.86 ms per token,    17.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    5317.95 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3126.76 ms /   314 tokens (    9.96 ms per token,   100.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     213.41 ms /     4 runs   (   53.35 ms per token,    18.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3345.17 ms /   318 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3000.68 ms /   308 tokens (    9.74 ms per token,   102.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     488.19 ms /     9 runs   (   54.24 ms per token,    18.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    3496.95 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 260 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5319.82 ms /   260 tokens (   20.46 ms per token,    48.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.22 ms /     3 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    5558.34 ms /   263 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4538.87 ms /   244 tokens (   18.60 ms per token,    53.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     112.92 ms /     2 runs   (   56.46 ms per token,    17.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    4655.68 ms /   246 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2703.93 ms /   289 tokens (    9.36 ms per token,   106.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     229.17 ms /     4 runs   (   57.29 ms per token,    17.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    2938.14 ms /   293 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2908.56 ms /   312 tokens (    9.32 ms per token,   107.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     221.49 ms /     4 runs   (   55.37 ms per token,    18.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3134.79 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4728.00 ms /   381 tokens (   12.41 ms per token,    80.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     360.29 ms /     6 runs   (   60.05 ms per token,    16.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    5094.46 ms /   387 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2514.73 ms /   274 tokens (    9.18 ms per token,   108.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.00 ms /     4 runs   (   86.00 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.85 ms /   278 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2691.31 ms /   282 tokens (    9.54 ms per token,   104.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     537.09 ms /     9 runs   (   59.68 ms per token,    16.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    3236.12 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3565.66 ms /   395 tokens (    9.03 ms per token,   110.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     228.85 ms /     4 runs   (   57.21 ms per token,    17.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    3799.55 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3150.16 ms /   339 tokens (    9.29 ms per token,   107.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.86 ms /     6 runs   (   58.64 ms per token,    17.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3508.26 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 283 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2732.81 ms /   283 tokens (    9.66 ms per token,   103.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     500.19 ms /     9 runs   (   55.58 ms per token,    17.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3241.10 ms /   292 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3231.75 ms /   352 tokens (    9.18 ms per token,   108.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     231.72 ms /     4 runs   (   57.93 ms per token,    17.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3468.28 ms /   356 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3385.68 ms /   326 tokens (   10.39 ms per token,    96.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     265.60 ms /     4 runs   (   66.40 ms per token,    15.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    3656.20 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 280 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5234.09 ms /   280 tokens (   18.69 ms per token,    53.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.21 ms /     4 runs   (   79.55 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    5557.47 ms /   284 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 334 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4443.16 ms /   334 tokens (   13.30 ms per token,    75.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.30 ms /     4 runs   (   93.08 ms per token,    10.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    4821.88 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 351 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3969.69 ms /   351 tokens (   11.31 ms per token,    88.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     257.27 ms /     4 runs   (   64.32 ms per token,    15.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    4231.48 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2972.47 ms /   325 tokens (    9.15 ms per token,   109.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     717.20 ms /     7 runs   (  102.46 ms per token,     9.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    3697.57 ms /   332 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3119.64 ms /   326 tokens (    9.57 ms per token,   104.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     789.22 ms /     4 runs   (  197.31 ms per token,     5.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3914.33 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3027.74 ms /   285 tokens (   10.62 ms per token,    94.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     388.20 ms /     4 runs   (   97.05 ms per token,    10.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    3420.43 ms /   289 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3813.80 ms /   319 tokens (   11.96 ms per token,    83.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     733.76 ms /     3 runs   (  244.59 ms per token,     4.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    4551.72 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 359 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4519.59 ms /   359 tokens (   12.59 ms per token,    79.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.05 ms /     6 runs   (   50.67 ms per token,    19.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4829.33 ms /   365 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5060.37 ms /   319 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1679.06 ms /     9 runs   (  186.56 ms per token,     5.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    6747.99 ms /   328 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3532.71 ms /   304 tokens (   11.62 ms per token,    86.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     550.14 ms /     9 runs   (   61.13 ms per token,    16.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4090.56 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2923.81 ms /   282 tokens (   10.37 ms per token,    96.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     508.23 ms /     9 runs   (   56.47 ms per token,    17.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    3439.99 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 332 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5536.72 ms /   332 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     775.66 ms /     9 runs   (   86.18 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    6321.44 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5128.16 ms /   355 tokens (   14.45 ms per token,    69.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1377.28 ms /     5 runs   (  275.46 ms per token,     3.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    6511.08 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 322 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6470.37 ms /   322 tokens (   20.09 ms per token,    49.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1099.25 ms /     4 runs   (  274.81 ms per token,     3.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    7574.93 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6160.82 ms /   312 tokens (   19.75 ms per token,    50.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     623.61 ms /     4 runs   (  155.90 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    6790.72 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 334 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4637.90 ms /   334 tokens (   13.89 ms per token,    72.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     754.53 ms /     9 runs   (   83.84 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    5401.21 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3596.86 ms /   314 tokens (   11.45 ms per token,    87.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     520.71 ms /     9 runs   (   57.86 ms per token,    17.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4125.63 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 320 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3300.56 ms /   320 tokens (   10.31 ms per token,    96.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     285.14 ms /     5 runs   (   57.03 ms per token,    17.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3590.84 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 406 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3642.10 ms /   406 tokens (    8.97 ms per token,   111.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     321.28 ms /     6 runs   (   53.55 ms per token,    18.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    3968.96 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2489.83 ms /   287 tokens (    8.68 ms per token,   115.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     237.06 ms /     4 runs   (   59.27 ms per token,    16.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    2731.42 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3328.29 ms /   380 tokens (    8.76 ms per token,   114.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     282.56 ms /     5 runs   (   56.51 ms per token,    17.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    3616.36 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 368 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3369.60 ms /   368 tokens (    9.16 ms per token,   109.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     290.86 ms /     5 runs   (   58.17 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    3666.10 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3102.42 ms /   345 tokens (    8.99 ms per token,   111.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     269.72 ms /     5 runs   (   53.94 ms per token,    18.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3377.39 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2646.41 ms /   273 tokens (    9.69 ms per token,   103.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     257.64 ms /     4 runs   (   64.41 ms per token,    15.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    2908.86 ms /   277 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2365.93 ms /   266 tokens (    8.89 ms per token,   112.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     217.93 ms /     4 runs   (   54.48 ms per token,    18.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    2588.52 ms /   270 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2802.23 ms /   316 tokens (    8.87 ms per token,   112.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     230.95 ms /     4 runs   (   57.74 ms per token,    17.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    3037.92 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 335 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2980.55 ms /   335 tokens (    8.90 ms per token,   112.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     271.87 ms /     5 runs   (   54.37 ms per token,    18.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3257.94 ms /   340 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2695.88 ms /   298 tokens (    9.05 ms per token,   110.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.75 ms /     6 runs   (   55.62 ms per token,    17.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    3035.66 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2655.78 ms /   288 tokens (    9.22 ms per token,   108.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     528.94 ms /     9 runs   (   58.77 ms per token,    17.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    3192.34 ms /   297 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3353.89 ms /   361 tokens (    9.29 ms per token,   107.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     331.83 ms /     6 runs   (   55.30 ms per token,    18.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3692.03 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2932.10 ms /   326 tokens (    8.99 ms per token,   111.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     215.83 ms /     4 runs   (   53.96 ms per token,    18.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.52 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2857.37 ms /   304 tokens (    9.40 ms per token,   106.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     529.01 ms /     9 runs   (   58.78 ms per token,    17.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3394.25 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3354.70 ms /   347 tokens (    9.67 ms per token,   103.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     308.13 ms /     5 runs   (   61.63 ms per token,    16.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3668.34 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6472.73 ms /   412 tokens (   15.71 ms per token,    63.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     723.53 ms /     5 runs   (  144.71 ms per token,     6.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    7202.32 ms /   417 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 368 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3832.70 ms /   368 tokens (   10.41 ms per token,    96.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.89 ms /     6 runs   (   55.81 ms per token,    17.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    4174.19 ms /   374 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2922.71 ms /   277 tokens (   10.55 ms per token,    94.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     291.27 ms /     4 runs   (   72.82 ms per token,    13.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3219.07 ms /   281 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5123.46 ms /   346 tokens (   14.81 ms per token,    67.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     507.61 ms /     4 runs   (  126.90 ms per token,     7.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    5637.14 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 279 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3134.04 ms /   279 tokens (   11.23 ms per token,    89.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     250.36 ms /     3 runs   (   83.45 ms per token,    11.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    3389.73 ms /   282 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4336.56 ms /   281 tokens (   15.43 ms per token,    64.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2337.30 ms /     9 runs   (  259.70 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    6682.50 ms /   290 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3526.47 ms /   306 tokens (   11.52 ms per token,    86.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     976.49 ms /     4 runs   (  244.12 ms per token,     4.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    4508.42 ms /   310 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3721.43 ms /   311 tokens (   11.97 ms per token,    83.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     345.40 ms /     6 runs   (   57.57 ms per token,    17.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    4072.61 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2909.99 ms /   300 tokens (    9.70 ms per token,   103.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     405.75 ms /     4 runs   (  101.44 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    3320.46 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3286.02 ms /   366 tokens (    8.98 ms per token,   111.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     277.98 ms /     5 runs   (   55.60 ms per token,    17.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3569.30 ms /   371 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2978.07 ms /   298 tokens (    9.99 ms per token,   100.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     525.97 ms /     9 runs   (   58.44 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3511.45 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2903.47 ms /   318 tokens (    9.13 ms per token,   109.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     575.20 ms /     6 runs   (   95.87 ms per token,    10.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    3485.89 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3808.92 ms /   404 tokens (    9.43 ms per token,   106.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     202.71 ms /     4 runs   (   50.68 ms per token,    19.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4016.07 ms /   408 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3867.91 ms /   407 tokens (    9.50 ms per token,   105.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.90 ms /     5 runs   (   63.38 ms per token,    15.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    4190.78 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4044.20 ms /   389 tokens (   10.40 ms per token,    96.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     657.87 ms /     5 runs   (  131.57 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    4706.87 ms /   394 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3231.75 ms /   328 tokens (    9.85 ms per token,   101.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     273.97 ms /     5 runs   (   54.79 ms per token,    18.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3510.54 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3847.63 ms /   379 tokens (   10.15 ms per token,    98.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     523.65 ms /     9 runs   (   58.18 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4379.65 ms /   388 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3259.74 ms /   345 tokens (    9.45 ms per token,   105.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     258.02 ms /     5 runs   (   51.60 ms per token,    19.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3523.01 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3423.45 ms /   338 tokens (   10.13 ms per token,    98.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     172.52 ms /     3 runs   (   57.51 ms per token,    17.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    3600.30 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 406 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3911.72 ms /   406 tokens (    9.63 ms per token,   103.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     637.28 ms /     6 runs   (  106.21 ms per token,     9.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    4556.07 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 388 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4756.53 ms /   388 tokens (   12.26 ms per token,    81.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     401.59 ms /     7 runs   (   57.37 ms per token,    17.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    5163.55 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 104 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5378.21 ms /   321 tokens (   16.75 ms per token,    59.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     345.60 ms /     6 runs   (   57.60 ms per token,    17.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    5730.09 ms /   327 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3239.11 ms /   345 tokens (    9.39 ms per token,   106.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     604.98 ms /     9 runs   (   67.22 ms per token,    14.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.45 ms /   354 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7457.99 ms /   404 tokens (   18.46 ms per token,    54.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1221.06 ms /     4 runs   (  305.26 ms per token,     3.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    8685.26 ms /   408 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 368 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3953.63 ms /   368 tokens (   10.74 ms per token,    93.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     341.04 ms /     5 runs   (   68.21 ms per token,    14.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4300.55 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4391.94 ms /   277 tokens (   15.86 ms per token,    63.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     622.27 ms /     4 runs   (  155.57 ms per token,     6.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    5019.58 ms /   281 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6107.55 ms /   307 tokens (   19.89 ms per token,    50.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2397.54 ms /     9 runs   (  266.39 ms per token,     3.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    8515.46 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3938.26 ms /   287 tokens (   13.72 ms per token,    72.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     243.53 ms /     4 runs   (   60.88 ms per token,    16.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    4189.90 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 414 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3599.41 ms /   414 tokens (    8.69 ms per token,   115.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     403.62 ms /     7 runs   (   57.66 ms per token,    17.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    4010.15 ms /   421 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3380.06 ms /   325 tokens (   10.40 ms per token,    96.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     217.12 ms /     4 runs   (   54.28 ms per token,    18.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3602.33 ms /   329 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3881.64 ms /   311 tokens (   12.48 ms per token,    80.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     846.49 ms /     4 runs   (  211.62 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4735.06 ms /   315 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 334 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3446.33 ms /   334 tokens (   10.32 ms per token,    96.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     248.59 ms /     4 runs   (   62.15 ms per token,    16.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3699.76 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3640.71 ms /   339 tokens (   10.74 ms per token,    93.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1046.01 ms /     6 runs   (  174.33 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    4693.23 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 303 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4404.27 ms /   303 tokens (   14.54 ms per token,    68.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     214.62 ms /     4 runs   (   53.66 ms per token,    18.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    4623.54 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2865.85 ms /   316 tokens (    9.07 ms per token,   110.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     507.02 ms /     9 runs   (   56.34 ms per token,    17.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3380.59 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2904.52 ms /   315 tokens (    9.22 ms per token,   108.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     282.68 ms /     5 runs   (   56.54 ms per token,    17.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    3192.41 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2913.27 ms /   294 tokens (    9.91 ms per token,   100.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     244.40 ms /     4 runs   (   61.10 ms per token,    16.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.08 ms /   298 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5052.90 ms /   389 tokens (   12.99 ms per token,    76.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     263.42 ms /     5 runs   (   52.68 ms per token,    18.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    5321.66 ms /   394 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 367 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3463.71 ms /   367 tokens (    9.44 ms per token,   105.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     263.16 ms /     5 runs   (   52.63 ms per token,    19.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3732.11 ms /   372 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3158.90 ms /   340 tokens (    9.29 ms per token,   107.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     304.56 ms /     5 runs   (   60.91 ms per token,    16.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3468.76 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2993.57 ms /   340 tokens (    8.80 ms per token,   113.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     253.54 ms /     5 runs   (   50.71 ms per token,    19.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3251.85 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3473.86 ms /   389 tokens (    8.93 ms per token,   111.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     265.36 ms /     5 runs   (   53.07 ms per token,    18.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3744.77 ms /   394 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2803.39 ms /   315 tokens (    8.90 ms per token,   112.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     217.38 ms /     4 runs   (   54.34 ms per token,    18.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.77 ms /   319 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2983.45 ms /   318 tokens (    9.38 ms per token,   106.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     223.36 ms /     4 runs   (   55.84 ms per token,    17.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    3211.88 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2856.89 ms /   268 tokens (   10.66 ms per token,    93.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     163.87 ms /     3 runs   (   54.62 ms per token,    18.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.32 ms /   271 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2904.64 ms /   282 tokens (   10.30 ms per token,    97.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     219.33 ms /     4 runs   (   54.83 ms per token,    18.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3128.48 ms /   286 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2916.44 ms /   284 tokens (   10.27 ms per token,    97.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     901.22 ms /     5 runs   (  180.24 ms per token,     5.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3822.77 ms /   289 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3230.97 ms /   290 tokens (   11.14 ms per token,    89.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     219.15 ms /     4 runs   (   54.79 ms per token,    18.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3454.83 ms /   294 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2629.94 ms /   270 tokens (    9.74 ms per token,   102.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     216.43 ms /     4 runs   (   54.11 ms per token,    18.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.32 ms /   274 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3879.45 ms /   357 tokens (   10.87 ms per token,    92.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     394.06 ms /     6 runs   (   65.68 ms per token,    15.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4279.48 ms /   363 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3219.91 ms /   314 tokens (   10.25 ms per token,    97.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     410.49 ms /     7 runs   (   58.64 ms per token,    17.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3638.22 ms /   321 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3378.21 ms /   325 tokens (   10.39 ms per token,    96.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     218.43 ms /     4 runs   (   54.61 ms per token,    18.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3601.08 ms /   329 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2962.73 ms /   271 tokens (   10.93 ms per token,    91.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =     162.23 ms /     3 runs   (   54.08 ms per token,    18.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.10 ms /   274 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3390.93 ms /   352 tokens (    9.63 ms per token,   103.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     244.74 ms /     4 runs   (   61.18 ms per token,    16.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3640.53 ms /   356 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3082.01 ms /   340 tokens (    9.06 ms per token,   110.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =     474.03 ms /     4 runs   (  118.51 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    3560.88 ms /   344 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 331 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3086.32 ms /   331 tokens (    9.32 ms per token,   107.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.76 ms /     6 runs   (   54.79 ms per token,    18.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3421.74 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3791.82 ms /   319 tokens (   11.89 ms per token,    84.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     174.80 ms /     3 runs   (   58.27 ms per token,    17.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    3970.91 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4081.70 ms /   362 tokens (   11.28 ms per token,    88.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     751.51 ms /     6 runs   (  125.25 ms per token,     7.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    4839.39 ms /   368 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3074.00 ms /   328 tokens (    9.37 ms per token,   106.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     167.22 ms /     3 runs   (   55.74 ms per token,    17.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    3245.45 ms /   331 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 337 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3406.50 ms /   337 tokens (   10.11 ms per token,    98.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     817.34 ms /     4 runs   (  204.33 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    4228.81 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3439.79 ms /   338 tokens (   10.18 ms per token,    98.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     242.80 ms /     4 runs   (   60.70 ms per token,    16.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    3687.43 ms /   342 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4747.57 ms /   412 tokens (   11.52 ms per token,    86.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     336.64 ms /     6 runs   (   56.11 ms per token,    17.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    5090.12 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 101 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3855.81 ms /   342 tokens (   11.27 ms per token,    88.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.45 ms /     4 runs   (   55.61 ms per token,    17.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    4083.24 ms /   346 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2537.39 ms /   274 tokens (    9.26 ms per token,   107.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     790.69 ms /     9 runs   (   87.85 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3336.14 ms /   283 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3234.68 ms /   345 tokens (    9.38 ms per token,   106.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     219.40 ms /     4 runs   (   54.85 ms per token,    18.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3458.72 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3198.63 ms /   286 tokens (   11.18 ms per token,    89.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     201.46 ms /     4 runs   (   50.37 ms per token,    19.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3404.25 ms /   290 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 371 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3291.13 ms /   371 tokens (    8.87 ms per token,   112.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     213.52 ms /     4 runs   (   53.38 ms per token,    18.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3509.88 ms /   375 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 413 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4252.00 ms /   413 tokens (   10.30 ms per token,    97.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     277.03 ms /     5 runs   (   55.41 ms per token,    18.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    4534.36 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4325.91 ms /   399 tokens (   10.84 ms per token,    92.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     219.61 ms /     4 runs   (   54.90 ms per token,    18.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4549.52 ms /   403 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 322 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2906.29 ms /   322 tokens (    9.03 ms per token,   110.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     164.28 ms /     3 runs   (   54.76 ms per token,    18.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3074.89 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 367 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3407.87 ms /   367 tokens (    9.29 ms per token,   107.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     330.51 ms /     6 runs   (   55.09 ms per token,    18.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    3744.48 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3226.33 ms /   353 tokens (    9.14 ms per token,   109.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     239.19 ms /     4 runs   (   59.80 ms per token,    16.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    3470.25 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3102.85 ms /   347 tokens (    8.94 ms per token,   111.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     213.95 ms /     4 runs   (   53.49 ms per token,    18.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    3321.67 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 336 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3371.26 ms /   336 tokens (   10.03 ms per token,    99.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     170.72 ms /     3 runs   (   56.91 ms per token,    17.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    3546.29 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4375.39 ms /   448 tokens (    9.77 ms per token,   102.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     566.23 ms /     4 runs   (  141.56 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    4947.19 ms /   452 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3658.15 ms /   387 tokens (    9.45 ms per token,   105.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     280.10 ms /     5 runs   (   56.02 ms per token,    17.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    3943.13 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 388 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4036.70 ms /   388 tokens (   10.40 ms per token,    96.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     292.23 ms /     5 runs   (   58.45 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    4333.79 ms /   393 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3369.17 ms /   362 tokens (    9.31 ms per token,   107.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     271.76 ms /     5 runs   (   54.35 ms per token,    18.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    3646.57 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3277.49 ms /   321 tokens (   10.21 ms per token,    97.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     214.88 ms /     3 runs   (   71.63 ms per token,    13.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3496.77 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3299.95 ms /   330 tokens (   10.00 ms per token,   100.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     500.02 ms /     9 runs   (   55.56 ms per token,    18.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3807.80 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3217.66 ms /   348 tokens (    9.25 ms per token,   108.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     216.21 ms /     4 runs   (   54.05 ms per token,    18.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    3438.63 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2991.64 ms /   301 tokens (    9.94 ms per token,   100.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.74 ms /     4 runs   (   55.68 ms per token,    17.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3219.44 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3818.20 ms /   346 tokens (   11.04 ms per token,    90.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     111.33 ms /     2 runs   (   55.67 ms per token,    17.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3933.20 ms /   348 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 99 prefix-match hit, remaining 329 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3908.11 ms /   329 tokens (   11.88 ms per token,    84.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     970.74 ms /     4 runs   (  242.68 ms per token,     4.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    4885.00 ms /   333 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 420 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8169.62 ms /   420 tokens (   19.45 ms per token,    51.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.18 ms /     4 runs   (   55.55 ms per token,    18.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    8396.21 ms /   424 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4708.38 ms /   347 tokens (   13.57 ms per token,    73.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     276.57 ms /     4 runs   (   69.14 ms per token,    14.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    4989.97 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2731.05 ms /   281 tokens (    9.72 ms per token,   102.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     280.24 ms /     4 runs   (   70.06 ms per token,    14.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3016.09 ms /   285 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4033.33 ms /   366 tokens (   11.02 ms per token,    90.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =     629.07 ms /     9 runs   (   69.90 ms per token,    14.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4672.74 ms /   375 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 101 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3537.63 ms /   352 tokens (   10.05 ms per token,    99.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.75 ms /     5 runs   (   75.95 ms per token,    13.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    3923.10 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 417 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5315.34 ms /   417 tokens (   12.75 ms per token,    78.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     440.77 ms /     6 runs   (   73.46 ms per token,    13.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    5761.35 ms /   423 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3415.45 ms /   342 tokens (    9.99 ms per token,   100.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     777.86 ms /     9 runs   (   86.43 ms per token,    11.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    4202.23 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4059.79 ms /   408 tokens (    9.95 ms per token,   100.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     259.21 ms /     4 runs   (   64.80 ms per token,    15.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    4324.20 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3687.68 ms /   377 tokens (    9.78 ms per token,   102.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     264.23 ms /     4 runs   (   66.06 ms per token,    15.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3957.60 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3856.80 ms /   356 tokens (   10.83 ms per token,    92.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     560.98 ms /     4 runs   (  140.24 ms per token,     7.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    4422.98 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3482.12 ms /   353 tokens (    9.86 ms per token,   101.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     544.23 ms /     9 runs   (   60.47 ms per token,    16.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    4034.04 ms /   362 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4145.12 ms /   380 tokens (   10.91 ms per token,    91.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.89 ms /     5 runs   (   65.98 ms per token,    15.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    4480.96 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 428 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5475.10 ms /   428 tokens (   12.79 ms per token,    78.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     699.08 ms /     4 runs   (  174.77 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    6179.24 ms /   432 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5874.97 ms /   379 tokens (   15.50 ms per token,    64.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     749.14 ms /     6 runs   (  124.86 ms per token,     8.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    6632.06 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4301.60 ms /   364 tokens (   11.82 ms per token,    84.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     176.79 ms /     3 runs   (   58.93 ms per token,    16.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    4482.48 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 422 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4186.83 ms /   422 tokens (    9.92 ms per token,   100.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     329.91 ms /     6 runs   (   54.99 ms per token,    18.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4522.57 ms /   428 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4650.48 ms /   301 tokens (   15.45 ms per token,    64.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     248.22 ms /     4 runs   (   62.06 ms per token,    16.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    4903.82 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 403 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4714.60 ms /   403 tokens (   11.70 ms per token,    85.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =     234.35 ms /     4 runs   (   58.59 ms per token,    17.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    4953.00 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 396 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4965.18 ms /   396 tokens (   12.54 ms per token,    79.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     277.79 ms /     5 runs   (   55.56 ms per token,    18.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    5247.65 ms /   401 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   10476.76 ms /   468 tokens (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     633.34 ms /     7 runs   (   90.48 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   11117.33 ms /   475 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3079.43 ms /   308 tokens (   10.00 ms per token,   100.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     407.93 ms /     5 runs   (   81.59 ms per token,    12.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3492.78 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3247.11 ms /   345 tokens (    9.41 ms per token,   106.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     245.89 ms /     4 runs   (   61.47 ms per token,    16.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.76 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4024.50 ms /   408 tokens (    9.86 ms per token,   101.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.13 ms /     5 runs   (   57.63 ms per token,    17.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    4318.47 ms /   413 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 414 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3737.31 ms /   414 tokens (    9.03 ms per token,   110.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     652.47 ms /     6 runs   (  108.75 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4395.81 ms /   420 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 421 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4856.44 ms /   421 tokens (   11.54 ms per token,    86.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1528.36 ms /     4 runs   (  382.09 ms per token,     2.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    6389.98 ms /   425 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 378 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8395.61 ms /   378 tokens (   22.21 ms per token,    45.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     215.42 ms /     4 runs   (   53.86 ms per token,    18.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    8615.96 ms /   382 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 101 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2935.57 ms /   328 tokens (    8.95 ms per token,   111.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     680.15 ms /     9 runs   (   75.57 ms per token,    13.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3623.80 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 303 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2643.64 ms /   303 tokens (    8.72 ms per token,   114.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.89 ms /     5 runs   (   60.78 ms per token,    16.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    2952.89 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3570.42 ms /   310 tokens (   11.52 ms per token,    86.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     350.37 ms /     6 runs   (   58.40 ms per token,    17.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    3927.13 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 429 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5008.52 ms /   429 tokens (   11.67 ms per token,    85.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     261.09 ms /     4 runs   (   65.27 ms per token,    15.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5274.58 ms /   433 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 382 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3309.85 ms /   382 tokens (    8.66 ms per token,   115.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.48 ms /     6 runs   (   54.41 ms per token,    18.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    3642.57 ms /   388 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 378 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5403.90 ms /   378 tokens (   14.30 ms per token,    69.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     723.92 ms /     6 runs   (  120.65 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    6134.31 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2769.09 ms /   301 tokens (    9.20 ms per token,   108.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     216.60 ms /     4 runs   (   54.15 ms per token,    18.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    2990.39 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 332 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5031.91 ms /   332 tokens (   15.16 ms per token,    65.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     216.90 ms /     4 runs   (   54.22 ms per token,    18.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    5253.71 ms /   336 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 102 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4225.63 ms /   361 tokens (   11.71 ms per token,    85.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     481.71 ms /     9 runs   (   53.52 ms per token,    18.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    4715.47 ms /   370 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4095.86 ms /   326 tokens (   12.56 ms per token,    79.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     859.15 ms /     9 runs   (   95.46 ms per token,    10.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    4962.86 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 101 prefix-match hit, remaining 407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3509.01 ms /   407 tokens (    8.62 ms per token,   115.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     225.43 ms /     4 runs   (   56.36 ms per token,    17.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    3738.73 ms /   411 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4176.98 ms /   380 tokens (   10.99 ms per token,    90.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     231.78 ms /     4 runs   (   57.94 ms per token,    17.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    4413.74 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3515.08 ms /   354 tokens (    9.93 ms per token,   100.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     162.29 ms /     3 runs   (   54.10 ms per token,    18.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3681.90 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2897.36 ms /   312 tokens (    9.29 ms per token,   107.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     214.20 ms /     4 runs   (   53.55 ms per token,    18.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.42 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4222.45 ms /   401 tokens (   10.53 ms per token,    94.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     266.09 ms /     5 runs   (   53.22 ms per token,    18.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    4494.08 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 441 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3978.76 ms /   441 tokens (    9.02 ms per token,   110.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     206.65 ms /     4 runs   (   51.66 ms per token,    19.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4190.34 ms /   445 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3973.84 ms /   390 tokens (   10.19 ms per token,    98.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     375.57 ms /     6 runs   (   62.60 ms per token,    15.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    4354.19 ms /   396 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4427.31 ms /   381 tokens (   11.62 ms per token,    86.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     240.04 ms /     4 runs   (   60.01 ms per token,    16.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4672.14 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 292 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3364.76 ms /   292 tokens (   11.52 ms per token,    86.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     231.13 ms /     4 runs   (   57.78 ms per token,    17.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3600.58 ms /   296 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3313.75 ms /   309 tokens (   10.72 ms per token,    93.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     529.47 ms /     9 runs   (   58.83 ms per token,    17.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    3850.99 ms /   318 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2800.25 ms /   313 tokens (    8.95 ms per token,   111.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     851.42 ms /     9 runs   (   94.60 ms per token,    10.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    3660.26 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2725.21 ms /   267 tokens (   10.21 ms per token,    97.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     238.86 ms /     4 runs   (   59.71 ms per token,    16.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    2968.73 ms /   271 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5788.12 ms /   401 tokens (   14.43 ms per token,    69.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     391.77 ms /     5 runs   (   78.35 ms per token,    12.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    6185.29 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    9982.65 ms /   361 tokens (   27.65 ms per token,    36.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     655.99 ms /     7 runs   (   93.71 ms per token,    10.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   10646.13 ms /   368 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4055.88 ms /   328 tokens (   12.37 ms per token,    80.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     504.28 ms /     2 runs   (  252.14 ms per token,     3.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    4563.62 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 99 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3514.93 ms /   342 tokens (   10.28 ms per token,    97.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     517.81 ms /     5 runs   (  103.56 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4038.39 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2992.29 ms /   266 tokens (   11.25 ms per token,    88.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1391.00 ms /     9 runs   (  154.56 ms per token,     6.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    4391.48 ms /   275 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 407 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3957.97 ms /   407 tokens (    9.72 ms per token,   102.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.79 ms /     4 runs   (   75.95 ms per token,    13.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    4266.71 ms /   411 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 360 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3532.24 ms /   360 tokens (    9.81 ms per token,   101.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =     878.10 ms /     9 runs   (   97.57 ms per token,    10.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4418.27 ms /   369 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 423 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5034.36 ms /   423 tokens (   11.90 ms per token,    84.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =     426.40 ms /     6 runs   (   71.07 ms per token,    14.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    5465.89 ms /   429 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 384 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3869.67 ms /   384 tokens (   10.08 ms per token,    99.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     326.86 ms /     5 runs   (   65.37 ms per token,    15.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    4201.39 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4706.72 ms /   450 tokens (   10.46 ms per token,    95.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =     290.45 ms /     4 runs   (   72.61 ms per token,    13.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    5001.23 ms /   454 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3471.81 ms /   297 tokens (   11.69 ms per token,    85.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     629.06 ms /     9 runs   (   69.90 ms per token,    14.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    4108.83 ms /   306 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4115.61 ms /   348 tokens (   11.83 ms per token,    84.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     394.54 ms /     4 runs   (   98.63 ms per token,    10.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    4515.03 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4725.69 ms /   344 tokens (   13.74 ms per token,    72.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1259.43 ms /     6 runs   (  209.91 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    5991.74 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 403 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6395.47 ms /   403 tokens (   15.87 ms per token,    63.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.04 ms /     4 runs   (   75.76 ms per token,    13.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6702.90 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7646.21 ms /   352 tokens (   21.72 ms per token,    46.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     196.45 ms /     3 runs   (   65.48 ms per token,    15.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    7846.91 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 332 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4241.34 ms /   332 tokens (   12.78 ms per token,    78.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2045.48 ms /     9 runs   (  227.28 ms per token,     4.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    6295.87 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5941.07 ms /   392 tokens (   15.16 ms per token,    65.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2228.32 ms /     5 runs   (  445.66 ms per token,     2.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    8175.87 ms /   397 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 102 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3102.85 ms /   265 tokens (   11.71 ms per token,    85.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     548.00 ms /     9 runs   (   60.89 ms per token,    16.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3658.88 ms /   274 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 296 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4008.50 ms /   296 tokens (   13.54 ms per token,    73.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     455.59 ms /     7 runs   (   65.08 ms per token,    15.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4471.62 ms /   303 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 422 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5350.01 ms /   422 tokens (   12.68 ms per token,    78.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.33 ms /     5 runs   (   57.67 ms per token,    17.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    5643.12 ms /   427 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3113.59 ms /   339 tokens (    9.18 ms per token,   108.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     254.93 ms /     4 runs   (   63.73 ms per token,    15.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    3373.47 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 337 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3855.04 ms /   337 tokens (   11.44 ms per token,    87.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     342.44 ms /     6 runs   (   57.07 ms per token,    17.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    4204.64 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 101 prefix-match hit, remaining 373 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3338.73 ms /   373 tokens (    8.95 ms per token,   111.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     340.53 ms /     6 runs   (   56.76 ms per token,    17.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    3685.13 ms /   379 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 414 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3729.61 ms /   414 tokens (    9.01 ms per token,   111.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     112.17 ms /     2 runs   (   56.09 ms per token,    17.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.62 ms /   416 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3696.17 ms /   404 tokens (    9.15 ms per token,   109.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     276.07 ms /     5 runs   (   55.21 ms per token,    18.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3977.23 ms /   409 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3474.83 ms /   366 tokens (    9.49 ms per token,   105.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     357.17 ms /     6 runs   (   59.53 ms per token,    16.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3838.08 ms /   372 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3844.87 ms /   410 tokens (    9.38 ms per token,   106.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     237.34 ms /     4 runs   (   59.34 ms per token,    16.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.91 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2920.08 ms /   318 tokens (    9.18 ms per token,   108.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     224.30 ms /     4 runs   (   56.08 ms per token,    17.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3149.11 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3137.58 ms /   325 tokens (    9.65 ms per token,   103.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.59 ms /     5 runs   (   57.72 ms per token,    17.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.58 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2912.34 ms /   325 tokens (    8.96 ms per token,   111.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     173.56 ms /     3 runs   (   57.85 ms per token,    17.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    3090.29 ms /   328 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3320.53 ms /   356 tokens (    9.33 ms per token,   107.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     244.32 ms /     4 runs   (   61.08 ms per token,    16.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    3569.56 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3251.08 ms /   347 tokens (    9.37 ms per token,   106.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     526.71 ms /     9 runs   (   58.52 ms per token,    17.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    3786.65 ms /   356 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 391 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3627.76 ms /   391 tokens (    9.28 ms per token,   107.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     290.11 ms /     5 runs   (   58.02 ms per token,    17.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    3922.51 ms /   396 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 331 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3124.01 ms /   331 tokens (    9.44 ms per token,   105.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     384.78 ms /     6 runs   (   64.13 ms per token,    15.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    3514.85 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3351.88 ms /   361 tokens (    9.28 ms per token,   107.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.28 ms /     6 runs   (   57.38 ms per token,    17.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    3702.65 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2877.51 ms /   311 tokens (    9.25 ms per token,   108.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     514.63 ms /     9 runs   (   57.18 ms per token,    17.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3399.99 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3559.14 ms /   379 tokens (    9.39 ms per token,   106.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     228.52 ms /     4 runs   (   57.13 ms per token,    17.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    3792.69 ms /   383 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 375 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3534.01 ms /   375 tokens (    9.42 ms per token,   106.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     362.78 ms /     6 runs   (   60.46 ms per token,    16.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    3902.96 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 322 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3042.44 ms /   322 tokens (    9.45 ms per token,   105.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     222.37 ms /     4 runs   (   55.59 ms per token,    17.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    3269.55 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 415 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3847.25 ms /   415 tokens (    9.27 ms per token,   107.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     242.11 ms /     4 runs   (   60.53 ms per token,    16.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    4094.20 ms /   419 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2768.80 ms /   285 tokens (    9.72 ms per token,   102.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     181.89 ms /     3 runs   (   60.63 ms per token,    16.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    2954.81 ms /   288 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3283.78 ms /   361 tokens (    9.10 ms per token,   109.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     285.52 ms /     5 runs   (   57.10 ms per token,    17.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    3574.97 ms /   366 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2899.67 ms /   313 tokens (    9.26 ms per token,   107.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     224.23 ms /     4 runs   (   56.06 ms per token,    17.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.08 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3023.51 ms /   339 tokens (    8.92 ms per token,   112.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     138.25 ms /     2 runs   (   69.12 ms per token,    14.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    3165.36 ms /   341 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 99 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2869.07 ms /   310 tokens (    9.26 ms per token,   108.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     165.97 ms /     3 runs   (   55.32 ms per token,    18.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3039.20 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 384 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3595.91 ms /   384 tokens (    9.36 ms per token,   106.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     289.35 ms /     5 runs   (   57.87 ms per token,    17.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    3889.92 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3479.20 ms /   379 tokens (    9.18 ms per token,   108.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     343.67 ms /     6 runs   (   57.28 ms per token,    17.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    3829.27 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2835.14 ms /   306 tokens (    9.27 ms per token,   107.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     241.71 ms /     4 runs   (   60.43 ms per token,    16.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3082.20 ms /   310 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3725.52 ms /   404 tokens (    9.22 ms per token,   108.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =     399.69 ms /     7 runs   (   57.10 ms per token,    17.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    4131.90 ms /   411 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 262 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2520.40 ms /   262 tokens (    9.62 ms per token,   103.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     170.04 ms /     3 runs   (   56.68 ms per token,    17.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    2694.58 ms /   265 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3130.31 ms /   347 tokens (    9.02 ms per token,   110.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     277.44 ms /     4 runs   (   69.36 ms per token,    14.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    3412.76 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3230.83 ms /   343 tokens (    9.42 ms per token,   106.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =     159.99 ms /     3 runs   (   53.33 ms per token,    18.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    3395.17 ms /   346 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4365.66 ms /   482 tokens (    9.06 ms per token,   110.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     418.69 ms /     7 runs   (   59.81 ms per token,    16.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    4791.51 ms /   489 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2852.81 ms /   298 tokens (    9.57 ms per token,   104.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.83 ms /     4 runs   (   58.46 ms per token,    17.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    3091.19 ms /   302 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 409 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3670.39 ms /   409 tokens (    8.97 ms per token,   111.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     391.31 ms /     6 runs   (   65.22 ms per token,    15.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4068.11 ms /   415 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 333 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3022.71 ms /   333 tokens (    9.08 ms per token,   110.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     227.83 ms /     4 runs   (   56.96 ms per token,    17.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3255.20 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3413.88 ms /   356 tokens (    9.59 ms per token,   104.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     349.41 ms /     4 runs   (   87.35 ms per token,    11.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    3768.17 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4374.20 ms /   477 tokens (    9.17 ms per token,   109.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     413.85 ms /     6 runs   (   68.98 ms per token,    14.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    4794.13 ms /   483 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 291 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2711.26 ms /   291 tokens (    9.32 ms per token,   107.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     564.20 ms /     9 runs   (   62.69 ms per token,    15.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3283.79 ms /   300 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 296 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2757.92 ms /   296 tokens (    9.32 ms per token,   107.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.43 ms /     5 runs   (   57.69 ms per token,    17.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.00 ms /   301 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3463.37 ms /   354 tokens (    9.78 ms per token,   102.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     240.92 ms /     4 runs   (   60.23 ms per token,    16.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    3708.83 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3549.83 ms /   374 tokens (    9.49 ms per token,   105.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     236.05 ms /     4 runs   (   59.01 ms per token,    16.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    3790.85 ms /   378 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 384 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3556.22 ms /   384 tokens (    9.26 ms per token,   107.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     284.41 ms /     5 runs   (   56.88 ms per token,    17.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    3846.28 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 396 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3758.54 ms /   396 tokens (    9.49 ms per token,   105.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     339.63 ms /     6 runs   (   56.60 ms per token,    17.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    4103.91 ms /   402 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2636.00 ms /   288 tokens (    9.15 ms per token,   109.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     230.23 ms /     4 runs   (   57.56 ms per token,    17.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    2870.93 ms /   292 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3283.05 ms /   362 tokens (    9.07 ms per token,   110.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     248.01 ms /     4 runs   (   62.00 ms per token,    16.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    3535.87 ms /   366 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2876.86 ms /   310 tokens (    9.28 ms per token,   107.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     221.85 ms /     4 runs   (   55.46 ms per token,    18.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3103.94 ms /   314 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3030.20 ms /   326 tokens (    9.30 ms per token,   107.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     536.31 ms /     9 runs   (   59.59 ms per token,    16.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    3574.35 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3165.08 ms /   352 tokens (    8.99 ms per token,   111.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     557.79 ms /     9 runs   (   61.98 ms per token,    16.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    3730.80 ms /   361 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2750.44 ms /   299 tokens (    9.20 ms per token,   108.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     113.51 ms /     2 runs   (   56.76 ms per token,    17.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    2867.52 ms /   301 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 397 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3679.67 ms /   397 tokens (    9.27 ms per token,   107.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     339.87 ms /     6 runs   (   56.64 ms per token,    17.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    4026.22 ms /   403 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3224.31 ms /   342 tokens (    9.43 ms per token,   106.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     173.78 ms /     3 runs   (   57.93 ms per token,    17.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3402.31 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2972.38 ms /   317 tokens (    9.38 ms per token,   106.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     233.22 ms /     4 runs   (   58.30 ms per token,    17.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    3210.55 ms /   321 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2342.68 ms /   249 tokens (    9.41 ms per token,   106.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     265.80 ms /     4 runs   (   66.45 ms per token,    15.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    2613.25 ms /   253 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 349 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3225.88 ms /   349 tokens (    9.24 ms per token,   108.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     286.48 ms /     5 runs   (   57.30 ms per token,    17.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    3517.77 ms /   354 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3239.20 ms /   348 tokens (    9.31 ms per token,   107.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     226.75 ms /     4 runs   (   56.69 ms per token,    17.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    3470.75 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3248.56 ms /   345 tokens (    9.42 ms per token,   106.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     293.75 ms /     5 runs   (   58.75 ms per token,    17.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    3547.82 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2859.02 ms /   301 tokens (    9.50 ms per token,   105.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     162.25 ms /     3 runs   (   54.08 ms per token,    18.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    3025.52 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2934.05 ms /   311 tokens (    9.43 ms per token,   106.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     218.44 ms /     4 runs   (   54.61 ms per token,    18.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3157.53 ms /   315 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2730.43 ms /   282 tokens (    9.68 ms per token,   103.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =     274.15 ms /     5 runs   (   54.83 ms per token,    18.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    3009.89 ms /   287 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 338 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3098.28 ms /   338 tokens (    9.17 ms per token,   109.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     255.78 ms /     5 runs   (   51.16 ms per token,    19.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3359.44 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3060.72 ms /   325 tokens (    9.42 ms per token,   106.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     279.53 ms /     5 runs   (   55.91 ms per token,    17.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    3345.81 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4453.74 ms /   478 tokens (    9.32 ms per token,   107.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     358.04 ms /     6 runs   (   59.67 ms per token,    16.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    4818.46 ms /   484 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 623 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6070.69 ms /   623 tokens (    9.74 ms per token,   102.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     523.41 ms /     9 runs   (   58.16 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6601.14 ms /   632 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 671 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6261.08 ms /   671 tokens (    9.33 ms per token,   107.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =     322.87 ms /     5 runs   (   64.57 ms per token,    15.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    6589.46 ms /   676 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 565 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5376.53 ms /   565 tokens (    9.52 ms per token,   105.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     303.83 ms /     5 runs   (   60.77 ms per token,    16.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    5686.07 ms /   570 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 657 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6310.70 ms /   657 tokens (    9.61 ms per token,   104.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =     284.94 ms /     5 runs   (   56.99 ms per token,    17.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    6601.40 ms /   662 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3488.59 ms /   380 tokens (    9.18 ms per token,   108.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     512.05 ms /     9 runs   (   56.89 ms per token,    17.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    4008.48 ms /   389 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3582.57 ms /   395 tokens (    9.07 ms per token,   110.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     523.62 ms /     9 runs   (   58.18 ms per token,    17.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4113.66 ms /   404 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 350 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3593.89 ms /   350 tokens (   10.27 ms per token,    97.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     557.13 ms /     5 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    4157.09 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4735.56 ms /   474 tokens (    9.99 ms per token,   100.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     299.53 ms /     4 runs   (   74.88 ms per token,    13.35 tokens per second)\n",
      "llama_perf_context_print:       total time =    5040.32 ms /   478 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 426 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3981.92 ms /   426 tokens (    9.35 ms per token,   106.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.35 ms /     5 runs   (   70.27 ms per token,    14.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    4338.14 ms /   431 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6785.39 ms /   408 tokens (   16.63 ms per token,    60.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     281.78 ms /     4 runs   (   70.44 ms per token,    14.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    7072.17 ms /   412 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 421 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4352.16 ms /   421 tokens (   10.34 ms per token,    96.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     247.12 ms /     4 runs   (   61.78 ms per token,    16.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4603.26 ms /   425 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5014.49 ms /   346 tokens (   14.49 ms per token,    69.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2106.34 ms /     5 runs   (  421.27 ms per token,     2.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    7131.54 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8861.85 ms /   362 tokens (   24.48 ms per token,    40.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1631.17 ms /     9 runs   (  181.24 ms per token,     5.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   10503.61 ms /   371 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6684.88 ms /   354 tokens (   18.88 ms per token,    52.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     660.16 ms /     5 runs   (  132.03 ms per token,     7.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    7351.01 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   16881.09 ms /   401 tokens (   42.10 ms per token,    23.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1771.54 ms /     5 runs   (  354.31 ms per token,     2.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   18660.00 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5436.57 ms /   379 tokens (   14.34 ms per token,    69.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     429.71 ms /     5 runs   (   85.94 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    5872.76 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4306.34 ms /   282 tokens (   15.27 ms per token,    65.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1408.62 ms /     9 runs   (  156.51 ms per token,     6.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    5725.02 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 353 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5566.38 ms /   353 tokens (   15.77 ms per token,    63.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =     530.79 ms /     5 runs   (  106.16 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    6103.88 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 333 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4737.77 ms /   333 tokens (   14.23 ms per token,    70.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1043.05 ms /     9 runs   (  115.89 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    5789.05 ms /   342 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6449.98 ms /   345 tokens (   18.70 ms per token,    53.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     761.22 ms /     9 runs   (   84.58 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    7219.64 ms /   354 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 351 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4067.11 ms /   351 tokens (   11.59 ms per token,    86.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     548.89 ms /     7 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    4622.50 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 332 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3727.80 ms /   332 tokens (   11.23 ms per token,    89.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     377.49 ms /     5 runs   (   75.50 ms per token,    13.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4110.83 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3694.94 ms /   325 tokens (   11.37 ms per token,    87.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     661.28 ms /     9 runs   (   73.48 ms per token,    13.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    4364.64 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3370.47 ms /   315 tokens (   10.70 ms per token,    93.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     293.64 ms /     4 runs   (   73.41 ms per token,    13.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    3669.57 ms /   319 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4645.29 ms /   404 tokens (   11.50 ms per token,    86.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     402.15 ms /     5 runs   (   80.43 ms per token,    12.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    5052.58 ms /   409 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3874.64 ms /   348 tokens (   11.13 ms per token,    89.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     351.82 ms /     5 runs   (   70.36 ms per token,    14.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4231.99 ms /   353 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4242.92 ms /   395 tokens (   10.74 ms per token,    93.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     288.79 ms /     4 runs   (   72.20 ms per token,    13.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    4535.60 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4132.78 ms /   366 tokens (   11.29 ms per token,    88.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     359.17 ms /     5 runs   (   71.83 ms per token,    13.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    4497.96 ms /   371 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 375 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4155.82 ms /   375 tokens (   11.08 ms per token,    90.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.21 ms /     5 runs   (   75.84 ms per token,    13.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4540.47 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 360 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3981.14 ms /   360 tokens (   11.06 ms per token,    90.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     327.73 ms /     4 runs   (   81.93 ms per token,    12.21 tokens per second)\n",
      "llama_perf_context_print:       total time =    4313.91 ms /   364 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3746.10 ms /   302 tokens (   12.40 ms per token,    80.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     592.47 ms /     7 runs   (   84.64 ms per token,    11.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    4345.64 ms /   309 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3605.35 ms /   328 tokens (   10.99 ms per token,    90.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     754.09 ms /     9 runs   (   83.79 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    4367.92 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 334 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3804.64 ms /   334 tokens (   11.39 ms per token,    87.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     355.78 ms /     5 runs   (   71.16 ms per token,    14.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    4165.83 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 400 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4404.52 ms /   400 tokens (   11.01 ms per token,    90.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     374.37 ms /     5 runs   (   74.87 ms per token,    13.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4784.46 ms /   405 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 276 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3055.44 ms /   276 tokens (   11.07 ms per token,    90.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     382.17 ms /     5 runs   (   76.43 ms per token,    13.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3443.12 ms /   281 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 401 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4385.04 ms /   401 tokens (   10.94 ms per token,    91.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     473.91 ms /     6 runs   (   78.98 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4864.56 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3622.32 ms /   328 tokens (   11.04 ms per token,    90.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     641.50 ms /     9 runs   (   71.28 ms per token,    14.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    4271.80 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 447 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4988.62 ms /   447 tokens (   11.16 ms per token,    89.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     356.93 ms /     5 runs   (   71.39 ms per token,    14.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    5351.57 ms /   452 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4026.14 ms /   354 tokens (   11.37 ms per token,    87.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     301.86 ms /     4 runs   (   75.46 ms per token,    13.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    4333.21 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3361.35 ms /   274 tokens (   12.27 ms per token,    81.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =     355.25 ms /     5 runs   (   71.05 ms per token,    14.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    3722.62 ms /   279 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4058.63 ms /   323 tokens (   12.57 ms per token,    79.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     686.15 ms /     9 runs   (   76.24 ms per token,    13.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    4753.22 ms /   332 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3513.45 ms /   315 tokens (   11.15 ms per token,    89.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     355.04 ms /     5 runs   (   71.01 ms per token,    14.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    3874.53 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3478.21 ms /   300 tokens (   11.59 ms per token,    86.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     544.61 ms /     7 runs   (   77.80 ms per token,    12.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    4029.96 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3452.68 ms /   282 tokens (   12.24 ms per token,    81.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     434.16 ms /     5 runs   (   86.83 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    3893.85 ms /   287 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4824.53 ms /   390 tokens (   12.37 ms per token,    80.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     377.03 ms /     5 runs   (   75.41 ms per token,    13.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    5207.48 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4125.50 ms /   355 tokens (   11.62 ms per token,    86.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     505.86 ms /     6 runs   (   84.31 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    4637.86 ms /   361 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 418 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4756.46 ms /   418 tokens (   11.38 ms per token,    87.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     146.67 ms /     2 runs   (   73.34 ms per token,    13.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    4907.21 ms /   420 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4496.18 ms /   352 tokens (   12.77 ms per token,    78.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     348.79 ms /     4 runs   (   87.20 ms per token,    11.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    4850.62 ms /   356 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5116.78 ms /   416 tokens (   12.30 ms per token,    81.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =     439.53 ms /     6 runs   (   73.25 ms per token,    13.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    5563.72 ms /   422 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 357 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4485.24 ms /   357 tokens (   12.56 ms per token,    79.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     560.70 ms /     6 runs   (   93.45 ms per token,    10.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    5053.80 ms /   363 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3735.44 ms /   312 tokens (   11.97 ms per token,    83.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     414.77 ms /     5 runs   (   82.95 ms per token,    12.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    4156.95 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 329 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4003.35 ms /   329 tokens (   12.17 ms per token,    82.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     667.07 ms /     9 runs   (   74.12 ms per token,    13.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    4680.68 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4296.07 ms /   347 tokens (   12.38 ms per token,    80.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     366.08 ms /     5 runs   (   73.22 ms per token,    13.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    4668.17 ms /   352 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4333.66 ms /   354 tokens (   12.24 ms per token,    81.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     434.90 ms /     6 runs   (   72.48 ms per token,    13.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    4775.91 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 394 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4579.92 ms /   394 tokens (   11.62 ms per token,    86.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =     435.11 ms /     6 runs   (   72.52 ms per token,    13.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    5021.76 ms /   400 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3661.80 ms /   306 tokens (   11.97 ms per token,    83.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     646.92 ms /     9 runs   (   71.88 ms per token,    13.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    4316.97 ms /   315 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4370.47 ms /   374 tokens (   11.69 ms per token,    85.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     547.31 ms /     7 runs   (   78.19 ms per token,    12.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    4925.81 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3527.00 ms /   297 tokens (   11.88 ms per token,    84.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     792.60 ms /     9 runs   (   88.07 ms per token,    11.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4327.89 ms /   306 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4337.61 ms /   352 tokens (   12.32 ms per token,    81.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     467.82 ms /     6 runs   (   77.97 ms per token,    12.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    4812.47 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 385 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5911.73 ms /   385 tokens (   15.36 ms per token,    65.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1521.02 ms /     5 runs   (  304.20 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    7437.85 ms /   390 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 369 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    9566.50 ms /   369 tokens (   25.93 ms per token,    38.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     702.05 ms /     4 runs   (  175.51 ms per token,     5.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   10273.27 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5626.72 ms /   344 tokens (   16.36 ms per token,    61.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1958.08 ms /     5 runs   (  391.62 ms per token,     2.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    7590.25 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8500.37 ms /   315 tokens (   26.99 ms per token,    37.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5749.23 ms /     9 runs   (  638.80 ms per token,     1.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   14257.50 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    9870.31 ms /   342 tokens (   28.86 ms per token,    34.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10312.39 ms /     5 runs   ( 2062.48 ms per token,     0.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   20189.77 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8312.82 ms /   307 tokens (   27.08 ms per token,    36.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7623.99 ms /     6 runs   ( 1270.66 ms per token,     0.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   15942.81 ms /   313 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   12275.34 ms /   390 tokens (   31.48 ms per token,    31.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12421.39 ms /     5 runs   ( 2484.28 ms per token,     0.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   24703.33 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   10014.42 ms /   239 tokens (   41.90 ms per token,    23.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20765.96 ms /     9 runs   ( 2307.33 ms per token,     0.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   30787.36 ms /   248 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   10574.43 ms /   284 tokens (   37.23 ms per token,    26.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17617.56 ms /     9 runs   ( 1957.51 ms per token,     0.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   28199.95 ms /   293 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   10837.55 ms /   288 tokens (   37.63 ms per token,    26.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17129.67 ms /     8 runs   ( 2141.21 ms per token,     0.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   27973.96 ms /   296 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   12155.02 ms /   395 tokens (   30.77 ms per token,    32.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21272.95 ms /     9 runs   ( 2363.66 ms per token,     0.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   33434.97 ms /   404 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 378 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   12156.70 ms /   378 tokens (   32.16 ms per token,    31.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13968.66 ms /     6 runs   ( 2328.11 ms per token,     0.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   26132.07 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   11529.14 ms /   393 tokens (   29.34 ms per token,    34.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9087.02 ms /     4 runs   ( 2271.76 ms per token,     0.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   20621.00 ms /   397 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 369 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   11495.44 ms /   369 tokens (   31.15 ms per token,    32.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14902.33 ms /     6 runs   ( 2483.72 ms per token,     0.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   26403.59 ms /   375 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 292 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   11237.04 ms /   292 tokens (   38.48 ms per token,    25.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8760.92 ms /     5 runs   ( 1752.18 ms per token,     0.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   20003.31 ms /   297 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   11943.83 ms /   387 tokens (   30.86 ms per token,    32.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10983.90 ms /     4 runs   ( 2745.98 ms per token,     0.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   22932.85 ms /   391 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 345 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   11738.85 ms /   345 tokens (   34.03 ms per token,    29.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15020.14 ms /     8 runs   ( 1877.52 ms per token,     0.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   26766.69 ms /   353 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =   10723.84 ms /   328 tokens (   32.69 ms per token,    30.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15308.17 ms /     8 runs   ( 1913.52 ms per token,     0.52 tokens per second)\n",
      "llama_perf_context_print:       total time =   26039.30 ms /   336 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 427 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    9047.19 ms /   427 tokens (   21.19 ms per token,    47.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     978.55 ms /     5 runs   (  195.71 ms per token,     5.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   10032.51 ms /   432 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5733.88 ms /   312 tokens (   18.38 ms per token,    54.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1141.58 ms /     5 runs   (  228.32 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    6881.46 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 323 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5796.90 ms /   323 tokens (   17.95 ms per token,    55.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1103.53 ms /     9 runs   (  122.61 ms per token,     8.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    6909.18 ms /   332 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 428 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7394.89 ms /   428 tokens (   17.28 ms per token,    57.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     918.10 ms /     5 runs   (  183.62 ms per token,     5.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    8319.65 ms /   433 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6305.92 ms /   361 tokens (   17.47 ms per token,    57.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     887.94 ms /     5 runs   (  177.59 ms per token,     5.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    7199.72 ms /   366 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 428 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6805.50 ms /   428 tokens (   15.90 ms per token,    62.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     495.24 ms /     4 runs   (  123.81 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    7305.94 ms /   432 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6645.78 ms /   412 tokens (   16.13 ms per token,    61.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     410.23 ms /     6 runs   (   68.37 ms per token,    14.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    7061.86 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4253.09 ms /   416 tokens (   10.22 ms per token,    97.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     390.60 ms /     6 runs   (   65.10 ms per token,    15.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    4649.82 ms /   422 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 378 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5028.09 ms /   378 tokens (   13.30 ms per token,    75.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     601.08 ms /     9 runs   (   66.79 ms per token,    14.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    5638.80 ms /   387 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 331 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3463.07 ms /   331 tokens (   10.46 ms per token,    95.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     564.71 ms /     9 runs   (   62.75 ms per token,    15.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    4035.70 ms /   340 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4755.80 ms /   387 tokens (   12.29 ms per token,    81.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     306.43 ms /     5 runs   (   61.29 ms per token,    16.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    5066.99 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3668.01 ms /   310 tokens (   11.83 ms per token,    84.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1237.33 ms /     6 runs   (  206.22 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    4912.02 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5237.96 ms /   376 tokens (   13.93 ms per token,    71.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =     399.03 ms /     5 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    5643.00 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 101 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4492.89 ms /   376 tokens (   11.95 ms per token,    83.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =     464.81 ms /     5 runs   (   92.96 ms per token,    10.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    4963.35 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 358 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3037.68 ms /   358 tokens (    8.49 ms per token,   117.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     243.30 ms /     5 runs   (   48.66 ms per token,    20.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3286.50 ms /   363 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2236.97 ms /   269 tokens (    8.32 ms per token,   120.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     438.29 ms /     9 runs   (   48.70 ms per token,    20.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    2682.75 ms /   278 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3537.41 ms /   381 tokens (    9.28 ms per token,   107.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     239.03 ms /     4 runs   (   59.76 ms per token,    16.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3781.54 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 413 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3563.26 ms /   413 tokens (    8.63 ms per token,   115.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     274.95 ms /     5 runs   (   54.99 ms per token,    18.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    3842.77 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3521.04 ms /   390 tokens (    9.03 ms per token,   110.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     252.19 ms /     5 runs   (   50.44 ms per token,    19.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3778.16 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 395 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3355.45 ms /   395 tokens (    8.49 ms per token,   117.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     280.44 ms /     4 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3639.75 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 402 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3311.07 ms /   402 tokens (    8.24 ms per token,   121.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     315.78 ms /     5 runs   (   63.16 ms per token,    15.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3631.21 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3191.84 ms /   376 tokens (    8.49 ms per token,   117.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     227.90 ms /     4 runs   (   56.98 ms per token,    17.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    3424.22 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3421.33 ms /   410 tokens (    8.34 ms per token,   119.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     193.00 ms /     4 runs   (   48.25 ms per token,    20.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    3619.00 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3442.59 ms /   412 tokens (    8.36 ms per token,   119.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =     444.53 ms /     9 runs   (   49.39 ms per token,    20.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    3893.91 ms /   421 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2663.84 ms /   326 tokens (    8.17 ms per token,   122.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     444.33 ms /     9 runs   (   49.37 ms per token,    20.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    3115.85 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3350.61 ms /   374 tokens (    8.96 ms per token,   111.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     332.43 ms /     6 runs   (   55.40 ms per token,    18.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3689.04 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2485.97 ms /   299 tokens (    8.31 ms per token,   120.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =     592.19 ms /     9 runs   (   65.80 ms per token,    15.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    3086.48 ms /   308 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3497.85 ms /   411 tokens (    8.51 ms per token,   117.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     278.95 ms /     4 runs   (   69.74 ms per token,    14.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    3781.96 ms /   415 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 428 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3642.65 ms /   428 tokens (    8.51 ms per token,   117.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     333.85 ms /     6 runs   (   55.64 ms per token,    17.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3981.70 ms /   434 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2612.66 ms /   321 tokens (    8.14 ms per token,   122.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     190.16 ms /     4 runs   (   47.54 ms per token,    21.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    2807.21 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 320 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    2690.21 ms /   320 tokens (    8.41 ms per token,   118.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     432.06 ms /     9 runs   (   48.01 ms per token,    20.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.20 ms /   329 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 370 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3299.99 ms /   370 tokens (    8.92 ms per token,   112.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     367.75 ms /     5 runs   (   73.55 ms per token,    13.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    3673.13 ms /   375 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 365 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3566.46 ms /   365 tokens (    9.77 ms per token,   102.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     475.61 ms /     9 runs   (   52.85 ms per token,    18.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    4050.12 ms /   374 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 418 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5359.36 ms /   418 tokens (   12.82 ms per token,    77.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     398.32 ms /     5 runs   (   79.66 ms per token,    12.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    5763.99 ms /   423 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5340.84 ms /   387 tokens (   13.80 ms per token,    72.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     483.55 ms /     5 runs   (   96.71 ms per token,    10.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    5831.76 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4497.60 ms /   282 tokens (   15.95 ms per token,    62.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     874.24 ms /     9 runs   (   97.14 ms per token,    10.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    5381.18 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5160.35 ms /   408 tokens (   12.65 ms per token,    79.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     712.53 ms /     8 runs   (   89.07 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    5880.56 ms /   416 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4203.00 ms /   321 tokens (   13.09 ms per token,    76.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     771.20 ms /     9 runs   (   85.69 ms per token,    11.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    4983.00 ms /   330 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6052.89 ms /   344 tokens (   17.60 ms per token,    56.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =     834.19 ms /     5 runs   (  166.84 ms per token,     5.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    6893.78 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5177.92 ms /   346 tokens (   14.97 ms per token,    66.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1171.39 ms /     9 runs   (  130.15 ms per token,     7.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    6357.92 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 283 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5500.16 ms /   283 tokens (   19.44 ms per token,    51.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     383.72 ms /     5 runs   (   76.74 ms per token,    13.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    5889.51 ms /   288 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 431 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5681.77 ms /   431 tokens (   13.18 ms per token,    75.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     482.68 ms /     5 runs   (   96.54 ms per token,    10.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    6170.93 ms /   436 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 346 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5057.41 ms /   346 tokens (   14.62 ms per token,    68.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     461.76 ms /     5 runs   (   92.35 ms per token,    10.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    5525.36 ms /   351 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4921.01 ms /   328 tokens (   15.00 ms per token,    66.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     948.18 ms /     9 runs   (  105.35 ms per token,     9.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    5879.15 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 297 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4696.36 ms /   297 tokens (   15.81 ms per token,    63.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     704.59 ms /     7 runs   (  100.66 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    5410.01 ms /   304 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 442 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6281.50 ms /   442 tokens (   14.21 ms per token,    70.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =     419.77 ms /     4 runs   (  104.94 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    6707.52 ms /   446 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5484.52 ms /   354 tokens (   15.49 ms per token,    64.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     453.79 ms /     5 runs   (   90.76 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    5945.16 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 394 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6032.76 ms /   394 tokens (   15.31 ms per token,    65.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     533.22 ms /     5 runs   (  106.64 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    6573.04 ms /   399 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 383 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6245.89 ms /   383 tokens (   16.31 ms per token,    61.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =     490.74 ms /     5 runs   (   98.15 ms per token,    10.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6743.74 ms /   388 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5426.94 ms /   330 tokens (   16.45 ms per token,    60.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     981.68 ms /     9 runs   (  109.08 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    6418.92 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 427 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7123.24 ms /   427 tokens (   16.68 ms per token,    59.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     613.80 ms /     5 runs   (  122.76 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    7744.37 ms /   432 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5638.03 ms /   327 tokens (   17.24 ms per token,    58.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =     832.19 ms /     5 runs   (  166.44 ms per token,     6.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    6477.93 ms /   332 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5433.75 ms /   298 tokens (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     928.13 ms /     9 runs   (  103.13 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    6372.33 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 404 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6875.03 ms /   404 tokens (   17.02 ms per token,    58.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     649.06 ms /     5 runs   (  129.81 ms per token,     7.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    7532.51 ms /   409 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 350 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6336.65 ms /   350 tokens (   18.10 ms per token,    55.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =     734.11 ms /     5 runs   (  146.82 ms per token,     6.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    7080.19 ms /   355 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 341 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6007.94 ms /   341 tokens (   17.62 ms per token,    56.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =     606.20 ms /     6 runs   (  101.03 ms per token,     9.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    6623.62 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6575.47 ms /   366 tokens (   17.97 ms per token,    55.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     585.11 ms /     5 runs   (  117.02 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    7169.03 ms /   371 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 435 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7845.08 ms /   435 tokens (   18.03 ms per token,    55.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     698.20 ms /     6 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    8551.70 ms /   441 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7309.58 ms /   410 tokens (   17.83 ms per token,    56.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     583.86 ms /     5 runs   (  116.77 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    7901.85 ms /   415 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 371 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6859.78 ms /   371 tokens (   18.49 ms per token,    54.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1003.38 ms /     9 runs   (  111.49 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    7874.37 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 359 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6093.61 ms /   359 tokens (   16.97 ms per token,    58.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     544.12 ms /     5 runs   (  108.82 ms per token,     9.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6645.36 ms /   364 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6088.55 ms /   328 tokens (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1029.49 ms /     9 runs   (  114.39 ms per token,     8.74 tokens per second)\n",
      "llama_perf_context_print:       total time =    7129.65 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 351 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6411.32 ms /   351 tokens (   18.27 ms per token,    54.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     602.45 ms /     5 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    7022.81 ms /   356 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7298.56 ms /   393 tokens (   18.57 ms per token,    53.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =     659.22 ms /     5 runs   (  131.84 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    7965.56 ms /   398 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 442 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7745.81 ms /   442 tokens (   17.52 ms per token,    57.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     639.75 ms /     5 runs   (  127.95 ms per token,     7.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    8393.99 ms /   447 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6836.08 ms /   348 tokens (   19.64 ms per token,    50.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =     620.47 ms /     5 runs   (  124.09 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    7464.55 ms /   353 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8445.35 ms /   484 tokens (   17.45 ms per token,    57.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     985.10 ms /     8 runs   (  123.14 ms per token,     8.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    9441.92 ms /   492 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8443.07 ms /   476 tokens (   17.74 ms per token,    56.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     500.66 ms /     4 runs   (  125.17 ms per token,     7.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    8950.52 ms /   480 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 444 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7551.80 ms /   444 tokens (   17.01 ms per token,    58.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     416.16 ms /     4 runs   (  104.04 ms per token,     9.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    7973.32 ms /   448 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6429.02 ms /   352 tokens (   18.26 ms per token,    54.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     657.16 ms /     5 runs   (  131.43 ms per token,     7.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    7095.03 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6563.70 ms /   362 tokens (   18.13 ms per token,    55.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1010.48 ms /     9 runs   (  112.28 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    7586.06 ms /   371 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 445 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7781.98 ms /   445 tokens (   17.49 ms per token,    57.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     618.90 ms /     5 runs   (  123.78 ms per token,     8.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    8409.03 ms /   450 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7047.26 ms /   376 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     576.98 ms /     5 runs   (  115.40 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7631.84 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6882.69 ms /   392 tokens (   17.56 ms per token,    56.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1037.60 ms /     9 runs   (  115.29 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7930.16 ms /   401 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6604.59 ms /   377 tokens (   17.52 ms per token,    57.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     560.97 ms /     5 runs   (  112.19 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    7173.17 ms /   382 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5678.33 ms /   339 tokens (   16.75 ms per token,    59.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1071.12 ms /     9 runs   (  119.01 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    6759.93 ms /   348 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6182.57 ms /   330 tokens (   18.74 ms per token,    53.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     903.45 ms /     6 runs   (  150.58 ms per token,     6.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    7094.99 ms /   336 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6371.25 ms /   355 tokens (   17.95 ms per token,    55.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     574.32 ms /     5 runs   (  114.86 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =    6952.70 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6003.26 ms /   330 tokens (   18.19 ms per token,    54.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     831.46 ms /     7 runs   (  118.78 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:       total time =    6843.88 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6566.99 ms /   355 tokens (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =     613.64 ms /     5 runs   (  122.73 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    7188.70 ms /   360 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 417 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7316.47 ms /   417 tokens (   17.55 ms per token,    56.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     532.78 ms /     5 runs   (  106.56 ms per token,     9.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    7857.09 ms /   422 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5601.48 ms /   319 tokens (   17.56 ms per token,    56.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =     568.02 ms /     5 runs   (  113.60 ms per token,     8.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    6176.42 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    8369.94 ms /   461 tokens (   18.16 ms per token,    55.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     756.49 ms /     6 runs   (  126.08 ms per token,     7.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    9136.19 ms /   467 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 386 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7175.68 ms /   386 tokens (   18.59 ms per token,    53.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =     551.55 ms /     4 runs   (  137.89 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    7734.37 ms /   390 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7896.03 ms /   452 tokens (   17.47 ms per token,    57.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     789.45 ms /     6 runs   (  131.57 ms per token,     7.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    8693.96 ms /   458 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7240.49 ms /   416 tokens (   17.41 ms per token,    57.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     610.85 ms /     5 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    7858.23 ms /   421 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 375 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6989.79 ms /   375 tokens (   18.64 ms per token,    53.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     675.35 ms /     5 runs   (  135.07 ms per token,     7.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    7672.98 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6458.87 ms /   348 tokens (   18.56 ms per token,    53.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1053.91 ms /     9 runs   (  117.10 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    7523.24 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5278.32 ms /   294 tokens (   17.95 ms per token,    55.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     463.13 ms /     4 runs   (  115.78 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    5748.03 ms /   298 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5725.98 ms /   317 tokens (   18.06 ms per token,    55.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1067.30 ms /     9 runs   (  118.59 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:       total time =    6804.54 ms /   326 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5441.66 ms /   301 tokens (   18.08 ms per token,    55.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     884.79 ms /     8 runs   (  110.60 ms per token,     9.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    6336.19 ms /   309 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5597.78 ms /   307 tokens (   18.23 ms per token,    54.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =     940.68 ms /     9 runs   (  104.52 ms per token,     9.57 tokens per second)\n",
      "llama_perf_context_print:       total time =    6548.41 ms /   316 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 400 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6646.13 ms /   400 tokens (   16.62 ms per token,    60.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     791.55 ms /     6 runs   (  131.93 ms per token,     7.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    7446.09 ms /   406 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 366 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6782.37 ms /   366 tokens (   18.53 ms per token,    53.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =     839.95 ms /     7 runs   (  119.99 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    7631.43 ms /   373 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6383.64 ms /   374 tokens (   17.07 ms per token,    58.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =     696.56 ms /     6 runs   (  116.09 ms per token,     8.61 tokens per second)\n",
      "llama_perf_context_print:       total time =    7087.95 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6710.95 ms /   408 tokens (   16.45 ms per token,    60.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =     740.70 ms /     6 runs   (  123.45 ms per token,     8.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    7461.44 ms /   414 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6473.41 ms /   392 tokens (   16.51 ms per token,    60.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     473.26 ms /     4 runs   (  118.31 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    6951.88 ms /   396 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 393 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6548.85 ms /   393 tokens (   16.66 ms per token,    60.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     439.37 ms /     4 runs   (  109.84 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    6994.15 ms /   397 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4993.27 ms /   282 tokens (   17.71 ms per token,    56.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =     967.67 ms /     9 runs   (  107.52 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    5971.88 ms /   291 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 341 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5445.36 ms /   341 tokens (   15.97 ms per token,    62.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     558.90 ms /     5 runs   (  111.78 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    6011.52 ms /   346 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5377.69 ms /   340 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =     485.25 ms /     4 runs   (  121.31 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5869.08 ms /   344 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5523.55 ms /   328 tokens (   16.84 ms per token,    59.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =     833.51 ms /     7 runs   (  119.07 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    6365.76 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6215.84 ms /   387 tokens (   16.06 ms per token,    62.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     931.02 ms /     9 runs   (  103.45 ms per token,     9.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7157.36 ms /   396 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 423 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6785.58 ms /   423 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     457.30 ms /     4 runs   (  114.33 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    7249.24 ms /   427 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7368.64 ms /   455 tokens (   16.19 ms per token,    61.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     513.53 ms /     4 runs   (  128.38 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    7889.20 ms /   459 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5482.63 ms /   315 tokens (   17.41 ms per token,    57.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =     537.55 ms /     5 runs   (  107.51 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6027.41 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6087.01 ms /   376 tokens (   16.19 ms per token,    61.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =     536.84 ms /     5 runs   (  107.37 ms per token,     9.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    6630.47 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5557.36 ms /   326 tokens (   17.05 ms per token,    58.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =     978.30 ms /     9 runs   (  108.70 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6546.04 ms /   335 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 391 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6501.11 ms /   391 tokens (   16.63 ms per token,    60.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     688.02 ms /     6 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    7196.96 ms /   397 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5208.49 ms /   315 tokens (   16.53 ms per token,    60.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1076.47 ms /     9 runs   (  119.61 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    6295.58 ms /   324 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4924.62 ms /   326 tokens (   15.11 ms per token,    66.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     508.09 ms /     5 runs   (  101.62 ms per token,     9.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    5439.59 ms /   331 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6340.06 ms /   412 tokens (   15.39 ms per token,    64.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =     918.52 ms /     9 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    7267.71 ms /   421 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 398 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6146.95 ms /   398 tokens (   15.44 ms per token,    64.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =     927.79 ms /     9 runs   (  103.09 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    7083.65 ms /   407 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 419 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6425.22 ms /   419 tokens (   15.33 ms per token,    65.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =     583.99 ms /     5 runs   (  116.80 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    7014.96 ms /   424 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5502.05 ms /   344 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     531.82 ms /     5 runs   (  106.36 ms per token,     9.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    6041.34 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5069.84 ms /   339 tokens (   14.96 ms per token,    66.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     360.16 ms /     4 runs   (   90.04 ms per token,    11.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    5435.82 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4499.09 ms /   295 tokens (   15.25 ms per token,    65.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =     377.98 ms /     4 runs   (   94.49 ms per token,    10.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    4883.12 ms /   299 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 348 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5220.97 ms /   348 tokens (   15.00 ms per token,    66.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     633.85 ms /     6 runs   (  105.64 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    5862.44 ms /   354 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5199.72 ms /   328 tokens (   15.85 ms per token,    63.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =     938.04 ms /     9 runs   (  104.23 ms per token,     9.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    6147.85 ms /   337 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 321 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4827.10 ms /   321 tokens (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     590.72 ms /     6 runs   (   98.45 ms per token,    10.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    5425.54 ms /   327 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 99 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5230.06 ms /   328 tokens (   15.95 ms per token,    62.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =     332.44 ms /     3 runs   (  110.81 ms per token,     9.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    5568.11 ms /   331 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6162.99 ms /   376 tokens (   16.39 ms per token,    61.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =     760.74 ms /     7 runs   (  108.68 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    6932.01 ms /   383 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 320 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5212.40 ms /   320 tokens (   16.29 ms per token,    61.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =     508.87 ms /     5 runs   (  101.77 ms per token,     9.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    5728.66 ms /   325 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5627.52 ms /   352 tokens (   15.99 ms per token,    62.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     520.72 ms /     5 runs   (  104.14 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:       total time =    6155.44 ms /   357 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4814.25 ms /   301 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     359.34 ms /     4 runs   (   89.84 ms per token,    11.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    5179.91 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4602.45 ms /   307 tokens (   14.99 ms per token,    66.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     375.65 ms /     4 runs   (   93.91 ms per token,    10.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    4983.95 ms /   311 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 333 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5039.82 ms /   333 tokens (   15.13 ms per token,    66.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     859.99 ms /     9 runs   (   95.55 ms per token,    10.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    5909.84 ms /   342 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 99 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4273.69 ms /   289 tokens (   14.79 ms per token,    67.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     369.82 ms /     4 runs   (   92.46 ms per token,    10.82 tokens per second)\n",
      "llama_perf_context_print:       total time =    4649.07 ms /   293 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4574.06 ms /   315 tokens (   14.52 ms per token,    68.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =     464.03 ms /     5 runs   (   92.81 ms per token,    10.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    5044.59 ms /   320 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 388 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5646.21 ms /   388 tokens (   14.55 ms per token,    68.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     799.49 ms /     7 runs   (  114.21 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    6454.14 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 99 prefix-match hit, remaining 341 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5135.46 ms /   341 tokens (   15.06 ms per token,    66.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     522.05 ms /     5 runs   (  104.41 ms per token,     9.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    5664.14 ms /   346 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5340.72 ms /   372 tokens (   14.36 ms per token,    69.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     496.49 ms /     5 runs   (   99.30 ms per token,    10.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    5843.88 ms /   377 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5882.58 ms /   377 tokens (   15.60 ms per token,    64.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     514.27 ms /     5 runs   (  102.85 ms per token,     9.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    6403.36 ms /   382 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 99 prefix-match hit, remaining 377 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5754.20 ms /   377 tokens (   15.26 ms per token,    65.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =     591.53 ms /     6 runs   (   98.59 ms per token,    10.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    6353.24 ms /   383 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 347 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7575.40 ms /   347 tokens (   21.83 ms per token,    45.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     187.70 ms /     3 runs   (   62.57 ms per token,    15.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    7767.98 ms /   350 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5932.14 ms /   466 tokens (   12.73 ms per token,    78.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =     379.45 ms /     6 runs   (   63.24 ms per token,    15.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    6317.80 ms /   472 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 330 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3539.02 ms /   330 tokens (   10.72 ms per token,    93.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =     625.09 ms /     9 runs   (   69.45 ms per token,    14.40 tokens per second)\n",
      "llama_perf_context_print:       total time =    4172.91 ms /   339 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5033.26 ms /   387 tokens (   13.01 ms per token,    76.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.60 ms /     5 runs   (   66.92 ms per token,    14.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    5372.67 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4400.78 ms /   315 tokens (   13.97 ms per token,    71.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =     618.31 ms /     3 runs   (  206.10 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    5023.75 ms /   318 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4861.70 ms /   387 tokens (   12.56 ms per token,    79.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =     390.99 ms /     5 runs   (   78.20 ms per token,    12.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    5258.31 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 310 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4979.10 ms /   310 tokens (   16.06 ms per token,    62.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =     730.30 ms /     4 runs   (  182.58 ms per token,     5.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    5714.48 ms /   314 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5280.84 ms /   319 tokens (   16.55 ms per token,    60.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =     488.06 ms /     3 runs   (  162.69 ms per token,     6.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    5773.00 ms /   322 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 423 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6812.62 ms /   423 tokens (   16.11 ms per token,    62.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =     815.42 ms /     7 runs   (  116.49 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    7634.73 ms /   430 tokens\n",
      "llama_perf_context_print:    graphs reused =          6\n",
      "Llama.generate: 100 prefix-match hit, remaining 335 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4781.05 ms /   335 tokens (   14.27 ms per token,    70.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     334.76 ms /     5 runs   (   66.95 ms per token,    14.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    5121.71 ms /   340 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 409 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4044.78 ms /   409 tokens (    9.89 ms per token,   101.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =     541.35 ms /     9 runs   (   60.15 ms per token,    16.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    4594.97 ms /   418 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4167.86 ms /   379 tokens (   11.00 ms per token,    90.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     310.39 ms /     5 runs   (   62.08 ms per token,    16.11 tokens per second)\n",
      "llama_perf_context_print:       total time =    4484.31 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 349 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4259.23 ms /   349 tokens (   12.20 ms per token,    81.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     517.82 ms /     9 runs   (   57.54 ms per token,    17.38 tokens per second)\n",
      "llama_perf_context_print:       total time =    4785.77 ms /   358 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 350 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3529.84 ms /   350 tokens (   10.09 ms per token,    99.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =     523.15 ms /     9 runs   (   58.13 ms per token,    17.20 tokens per second)\n",
      "llama_perf_context_print:       total time =    4061.30 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3091.45 ms /   299 tokens (   10.34 ms per token,    96.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     234.12 ms /     3 runs   (   78.04 ms per token,    12.81 tokens per second)\n",
      "llama_perf_context_print:       total time =    3330.91 ms /   302 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 100 prefix-match hit, remaining 384 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    3863.03 ms /   384 tokens (   10.06 ms per token,    99.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     387.20 ms /     6 runs   (   64.53 ms per token,    15.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    4256.54 ms /   390 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6049.52 ms /   372 tokens (   16.26 ms per token,    61.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =     406.60 ms /     5 runs   (   81.32 ms per token,    12.30 tokens per second)\n",
      "llama_perf_context_print:       total time =    6462.81 ms /   377 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 335 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4965.69 ms /   335 tokens (   14.82 ms per token,    67.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =     891.16 ms /     9 runs   (   99.02 ms per token,    10.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    5867.94 ms /   344 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 333 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4663.94 ms /   333 tokens (   14.01 ms per token,    71.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =     444.99 ms /     5 runs   (   89.00 ms per token,    11.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    5114.96 ms /   338 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4722.91 ms /   340 tokens (   13.89 ms per token,    71.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =     562.37 ms /     5 runs   (  112.47 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    5292.07 ms /   345 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 104 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5011.18 ms /   339 tokens (   14.78 ms per token,    67.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =     436.23 ms /     4 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =    5453.46 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 373 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5597.63 ms /   373 tokens (   15.01 ms per token,    66.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =     862.31 ms /     9 runs   (   95.81 ms per token,    10.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    6469.14 ms /   382 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 100 prefix-match hit, remaining 378 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5487.41 ms /   378 tokens (   14.52 ms per token,    68.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     590.79 ms /     5 runs   (  118.16 ms per token,     8.46 tokens per second)\n",
      "llama_perf_context_print:       total time =    6084.82 ms /   383 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 375 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5748.37 ms /   375 tokens (   15.33 ms per token,    65.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =     480.21 ms /     5 runs   (   96.04 ms per token,    10.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    6235.52 ms /   380 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5746.68 ms /   376 tokens (   15.28 ms per token,    65.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     490.77 ms /     5 runs   (   98.15 ms per token,    10.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    6244.08 ms /   381 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5771.20 ms /   364 tokens (   15.85 ms per token,    63.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =     556.78 ms /     5 runs   (  111.36 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    6335.08 ms /   369 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6238.28 ms /   389 tokens (   16.04 ms per token,    62.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     553.78 ms /     5 runs   (  110.76 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    6799.06 ms /   394 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 420 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7078.32 ms /   420 tokens (   16.85 ms per token,    59.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     453.01 ms /     4 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    7538.61 ms /   424 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 371 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6408.72 ms /   371 tokens (   17.27 ms per token,    57.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     630.75 ms /     6 runs   (  105.13 ms per token,     9.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    7048.14 ms /   377 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 380 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6595.18 ms /   380 tokens (   17.36 ms per token,    57.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =     576.96 ms /     5 runs   (  115.39 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    7180.91 ms /   385 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 355 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6047.66 ms /   355 tokens (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =     719.46 ms /     6 runs   (  119.91 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:       total time =    6775.14 ms /   361 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5016.57 ms /   285 tokens (   17.60 ms per token,    56.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =     588.71 ms /     5 runs   (  117.74 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =    5613.32 ms /   290 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 416 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7039.38 ms /   416 tokens (   16.92 ms per token,    59.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =     980.51 ms /     9 runs   (  108.95 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    8029.67 ms /   425 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6560.99 ms /   389 tokens (   16.87 ms per token,    59.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =     744.44 ms /     6 runs   (  124.07 ms per token,     8.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    7314.18 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6924.45 ms /   387 tokens (   17.89 ms per token,    55.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =     568.97 ms /     5 runs   (  113.79 ms per token,     8.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    7499.91 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6591.75 ms /   379 tokens (   17.39 ms per token,    57.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =     624.62 ms /     5 runs   (  124.92 ms per token,     8.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    7224.51 ms /   384 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 342 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6411.98 ms /   342 tokens (   18.75 ms per token,    53.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =     788.12 ms /     7 runs   (  112.59 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    7210.31 ms /   349 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6080.40 ms /   356 tokens (   17.08 ms per token,    58.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1062.89 ms /     9 runs   (  118.10 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    7154.72 ms /   365 tokens\n",
      "llama_perf_context_print:    graphs reused =          8\n",
      "Llama.generate: 100 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5382.78 ms /   314 tokens (   17.14 ms per token,    58.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     316.73 ms /     3 runs   (  105.58 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    5704.96 ms /   317 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5198.81 ms /   305 tokens (   17.05 ms per token,    58.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =     497.58 ms /     4 runs   (  124.39 ms per token,     8.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    5703.44 ms /   309 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 343 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6057.29 ms /   343 tokens (   17.66 ms per token,    56.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     446.15 ms /     4 runs   (  111.54 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    6509.79 ms /   347 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5513.91 ms /   309 tokens (   17.84 ms per token,    56.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =     577.69 ms /     5 runs   (  115.54 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    6099.24 ms /   314 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 341 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6299.26 ms /   341 tokens (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =     599.96 ms /     5 runs   (  119.99 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    6906.76 ms /   346 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    4285.78 ms /   256 tokens (   16.74 ms per token,    59.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =     239.05 ms /     2 runs   (  119.53 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:       total time =    4529.51 ms /   258 tokens\n",
      "llama_perf_context_print:    graphs reused =          1\n",
      "Llama.generate: 100 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6958.08 ms /   390 tokens (   17.84 ms per token,    56.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =     650.00 ms /     5 runs   (  130.00 ms per token,     7.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    7615.76 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 100 prefix-match hit, remaining 328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5766.98 ms /   328 tokens (   17.58 ms per token,    56.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =     759.89 ms /     6 runs   (  126.65 ms per token,     7.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    6536.22 ms /   334 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6600.13 ms /   339 tokens (   19.47 ms per token,    51.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =     698.72 ms /     4 runs   (  174.68 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    7305.56 ms /   343 tokens\n",
      "llama_perf_context_print:    graphs reused =          3\n",
      "Llama.generate: 100 prefix-match hit, remaining 433 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7420.58 ms /   433 tokens (   17.14 ms per token,    58.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =     641.88 ms /     5 runs   (  128.38 ms per token,     7.79 tokens per second)\n",
      "llama_perf_context_print:       total time =    8071.09 ms /   438 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    7269.78 ms /   411 tokens (   17.69 ms per token,    56.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =     656.27 ms /     6 runs   (  109.38 ms per token,     9.14 tokens per second)\n",
      "llama_perf_context_print:       total time =    7934.56 ms /   417 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5519.00 ms /   302 tokens (   18.27 ms per token,    54.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =     347.57 ms /     3 runs   (  115.86 ms per token,     8.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    5872.17 ms /   305 tokens\n",
      "llama_perf_context_print:    graphs reused =          2\n",
      "Llama.generate: 99 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6971.48 ms /   390 tokens (   17.88 ms per token,    55.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =     578.99 ms /     5 runs   (  115.80 ms per token,     8.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    7557.68 ms /   395 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 344 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5531.53 ms /   344 tokens (   16.08 ms per token,    62.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =     958.35 ms /     9 runs   (  106.48 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    6501.84 ms /   353 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5215.89 ms /   314 tokens (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =     958.18 ms /     9 runs   (  106.46 ms per token,     9.39 tokens per second)\n",
      "llama_perf_context_print:       total time =    6184.04 ms /   323 tokens\n",
      "llama_perf_context_print:    graphs reused =          7\n",
      "Llama.generate: 99 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5665.35 ms /   362 tokens (   15.65 ms per token,    63.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =     480.48 ms /     5 runs   (   96.10 ms per token,    10.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    6153.08 ms /   367 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 386 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6662.99 ms /   386 tokens (   17.26 ms per token,    57.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =     650.08 ms /     6 runs   (  108.35 ms per token,     9.23 tokens per second)\n",
      "llama_perf_context_print:       total time =    7321.02 ms /   392 tokens\n",
      "llama_perf_context_print:    graphs reused =          5\n",
      "Llama.generate: 100 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    5134.68 ms /   302 tokens (   17.00 ms per token,    58.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =     605.34 ms /     5 runs   (  121.07 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    5746.77 ms /   307 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n",
      "Llama.generate: 99 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    3889.40 ms\n",
      "llama_perf_context_print: prompt eval time =    6070.60 ms /   354 tokens (   17.15 ms per token,    58.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =     563.33 ms /     5 runs   (  112.67 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    6641.91 ms /   359 tokens\n",
      "llama_perf_context_print:    graphs reused =          4\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 9) Classificar todo o dataset com RAG\n",
    "##############################################\n",
    "\n",
    "preds = []\n",
    "gold = df[\"label\"].tolist()\n",
    "\n",
    "for text in df[\"text\"]:\n",
    "    raw = classify(text)\n",
    "\n",
    "    # limpeza da saída\n",
    "    clean = raw.lower().replace(\"[\", \"\").replace(\"]\", \"\").strip()\n",
    "\n",
    "    # mapeamento para garantir categoria válida\n",
    "    match = None\n",
    "    for lab in LABELS:\n",
    "        if lab in clean:\n",
    "            match = lab\n",
    "            break\n",
    "\n",
    "    if match is None:\n",
    "        # fallback: tenta escolher a categoria mais provável\n",
    "        # (pode ajustar conforme o comportamento do modelo)\n",
    "        match = clean.split()[0] if clean.split() else \"romance\"\n",
    "\n",
    "        if match not in LABELS:\n",
    "            match = \"romance\"\n",
    "\n",
    "    preds.append(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285fe2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULTADOS DO CLASSIFICADOR RAG + Qwen 1.5B GGUF ===\n",
      "Acurácia: 0.5565\n",
      "Precisão (weighted): 0.6981\n",
      "Recall (weighted): 0.5565\n",
      "F1 (weighted): 0.5527\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    aventura     0.9744    0.3800    0.5468       200\n",
      "     romance     0.5655    0.8200    0.6694       200\n",
      "       scifi     0.3065    0.8472    0.4502        72\n",
      "      terror     0.6952    0.3650    0.4787       200\n",
      "\n",
      "    accuracy                         0.5565       672\n",
      "   macro avg     0.6354    0.6031    0.5363       672\n",
      "weighted avg     0.6981    0.5565    0.5527       672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 10) Métricas de desempenho\n",
    "##############################################\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "acc = accuracy_score(gold, preds)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(gold, preds, average=\"weighted\")\n",
    "\n",
    "print(\"\\n=== RESULTADOS DO CLASSIFICADOR RAG + Qwen 1.5B GGUF ===\")\n",
    "print(f\"Acurácia: {acc:.4f}\")\n",
    "print(f\"Precisão (weighted): {prec:.4f}\")\n",
    "print(f\"Recall (weighted): {rec:.4f}\")\n",
    "print(f\"F1 (weighted): {f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(gold, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ff32f",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e1bcc",
   "metadata": {},
   "source": [
    "O presente projeto demonstrou, de forma prática, como modelos de linguagem de última geração (LLMs), combinados com técnicas de Recuperação Aumentada por Geração (RAG), podem melhorar significativamente a tarefa de classificação de gênero literário em textos longos e heterogêneos do Project Gutenberg.\n",
    "\n",
    "A primeira abordagem, baseada no uso direto de um modelo LLM em formato GGUF (Qwen2.5-1.5B-Instruct) operando localmente, mostrou que, embora LLMs sejam capazes de capturar padrões semânticos complexos, seu desempenho é limitado quando dependem exclusivamente do conhecimento contido nos pesos do modelo — especialmente para textos clássicos extensos e com grande variação de estilo.\n",
    "\n",
    "A introdução da arquitetura RAG, utilizando embeddings (via SentenceTransformers) e um índice vetorial FAISS para recuperação contextual, resultou em ganhos claros no processo de classificação. Ao fornecer exemplos semanticamente próximos como suporte contextual, o modelo conseguiu realizar inferências mais consistentes e alinhadas ao gênero real das obras. Assim, o pipeline híbrido (Embeddings → FAISS → Prompt com Exemplos → LLM GGUF) mostrou-se superior à classificação direta via LLM, confirmando resultados reportados na literatura, segundo os quais RAG melhora a precisão de modelos, reduz alucinações e aumenta a robustez em tarefas de classificação, QA e análise textual.\n",
    "\n",
    "Além disso, a adoção de modelos GGUF possibilitou executar inferência local com baixo custo computacional, garantindo privacidade total dos dados e permitindo replicação em ambientes com hardware limitado. Esse aspecto reforça a relevância prática de técnicas como quantização, já amplamente discutidas em estudos recentes focados em otimização e compressão de LLMs.\n",
    "\n",
    "O projeto, portanto, evidencia que:\n",
    "\n",
    "- LLMs quantizados são suficientes para tarefas de NLP quando combinados com técnicas adicionais, mesmo em máquinas pessoais;\n",
    "- RAG tende a superar abordagens puramente baseadas em geração, especialmente quando a tarefa depende de contexto específico, como no caso de gêneros literários;\n",
    "- O emprego de embeddings adequados e mecanismos eficientes de busca vetorial é crucial para elevar a qualidade da inferência contextual.\n",
    "\n",
    "Os resultados obtidos estão alinhados com pesquisas contemporâneas sobre RAG e LLMs, incluindo estudos de Lewis et al. (2020) sobre Retrieval-Augmented Generation, Reimers & Gurevych (2019) sobre Sentence-BERT e investigações recentes sobre modelos compactos e quantizados, como Dettmers et al. (2023). Assim, o trabalho reforça a relevância do uso de arquiteturas híbridas para melhorar a confiabilidade e a precisão de modelos de linguagem em problemas aplicados de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd89bd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
